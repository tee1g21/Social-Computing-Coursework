{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset from database\n",
    "def load_data(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Loaded database\")\n",
    "\n",
    "    c = conn.cursor()\n",
    "    print(\"Fetching data ...\")\n",
    "    c.execute('SELECT UserID, ItemID, Rating FROM example_table')\n",
    "    data = c.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    user_indices = []\n",
    "    item_indices = []\n",
    "    ratings_values = []\n",
    "\n",
    "    max_user_id = 0\n",
    "    max_item_id = 0\n",
    "\n",
    "    for user_id, item_id, rating in data:\n",
    "        user_indices.append(user_id)\n",
    "        item_indices.append(item_id)\n",
    "        ratings_values.append(rating)\n",
    "        #ratings_values.append(int(rating * 2)) # convert ratings to integers\n",
    "        max_user_id = max(max_user_id, user_id)\n",
    "        max_item_id = max(max_item_id, item_id)\n",
    "\n",
    "    user_indices = np.array(user_indices, dtype=np.int32)\n",
    "    item_indices = np.array(item_indices, dtype=np.int32)\n",
    "    #ratings_values = np.array(ratings_values, dtype=np.int32)\n",
    "    ratings_values = np.array(ratings_values, dtype=np.float32)\n",
    "\n",
    "    print(\"Max user id:\", max_user_id)\n",
    "    print(\"Max item id:\", max_item_id)\n",
    "\n",
    "    return user_indices, item_indices, ratings_values, max_user_id, max_item_id\n",
    "\n",
    "def normalize_ids(indices):\n",
    "    unique_ids, inverse_indices = np.unique(indices, return_inverse=True)\n",
    "    id_map = {original_id: idx for idx, original_id in enumerate(unique_ids)}\n",
    "    reverse_map = {idx: original_id for idx, original_id in enumerate(unique_ids)}\n",
    "    num_unique = len(unique_ids)  # The total number of unique indices\n",
    "    return inverse_indices, num_unique, id_map, reverse_map\n",
    "\n",
    "\n",
    "\n",
    "# split into train and validation sets\n",
    "def split_data(user_indices, item_indices, ratings, split_ratio=0.9):\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(ratings))\n",
    "    split_point = int(len(ratings) * split_ratio)\n",
    "    train_idx, val_idx = indices[:split_point], indices[split_point:]\n",
    "    \n",
    "    train_data = (user_indices[train_idx], item_indices[train_idx], ratings[train_idx])\n",
    "    val_data = (user_indices[val_idx], item_indices[val_idx], ratings[val_idx])\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded database\n",
      "Fetching data ...\n",
      "Max user id: 138493\n",
      "Max item id: 26744\n",
      "Train data size: 16753799\n",
      "Validation data size: 1861534\n"
     ]
    }
   ],
   "source": [
    "# impor dataset\n",
    "\n",
    "path_100k = '../../data/dataset1/train_100k.db'\n",
    "path_20M = '../../data/dataset2/train_20M.db'\n",
    "\n",
    "global_user_indices, global_item_indices, global_ratings, global_max_user_id, global_max_item_id = load_data(path_20M)\n",
    "\n",
    "# Normalize user and item indices\n",
    "global_user_indices, global_num_users, user_to_norm, norm_to_user = normalize_ids(global_user_indices)\n",
    "global_item_indices, global_num_items, item_to_norm, norm_to_item = normalize_ids(global_item_indices)\n",
    "\n",
    "all_data = (global_user_indices, global_item_indices, global_ratings)\n",
    "train_data, val_data = split_data(global_user_indices, global_item_indices, global_ratings, split_ratio=0.9)\n",
    "\n",
    "print(\"Train data size:\",train_data[0].size)\n",
    "print(\"Validation data size:\",val_data[0].size)\n",
    "\n",
    "# train_data[0] = user_indices\n",
    "# train_data[1] = item_indices\n",
    "# train_data[2] = ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE and Prediction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE and predict methods\n",
    "def calculate_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - actual_ratings: np.array, the actual ratings.\n",
    "    - predicted_ratings: np.array, the predicted ratings.\n",
    "    \"\"\"\n",
    "    # calculate the absolute error between actual and predicted ratings\n",
    "    abs_err = np.abs(actual - predicted)\n",
    "    \n",
    "    # calculate the mean of these absolute errors\n",
    "    mae = np.mean(abs_err)\n",
    "    \n",
    "    return mae # /2\n",
    "\n",
    "# round prediction to nearest 0.5 in range [0.5, 5]\n",
    "def round_predictions(predictions):\n",
    "    rounded_predictions = np.round(predictions * 2) / 2\n",
    "    return np.clip(rounded_predictions, 0.5, 5.0)\n",
    "    #rounded_predictions = np.round(predictions)\n",
    "    #return np.clip(rounded_predictions, 1, 10)\n",
    "\n",
    "def predict(user_features, item_features, user_indices, item_indices):\n",
    "    predictions = np.array([np.dot(user_features[u], item_features[i]) for u, i in zip(user_indices, item_indices)])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(user_indices, item_indices, ratings, num_users, num_items, num_factors, alpha, beta, iterations):\n",
    "    # Initialize feature matrices\n",
    "    np.random.seed(42)\n",
    "    user_features = np.random.normal(0, 0.1, (num_users, num_factors))\n",
    "    item_features = np.random.normal(0, 0.1, (num_items, num_factors))\n",
    "\n",
    "    # SGD updates (only using training data)\n",
    "    for iteration in range(iterations): #tqdm(range(iterations), desc='SGD iterations', total=iterations):\n",
    "        for u, i, r in tqdm(zip(user_indices, item_indices, ratings), desc=f'SGD {iteration+1}/{iterations}', total=len(ratings)):\n",
    "            prediction = np.dot(user_features[u], item_features[i])\n",
    "            error = r - prediction\n",
    "\n",
    "            # Update rules for features\n",
    "            user_features_grad = -2 * error * item_features[i] + beta * user_features[u]\n",
    "            item_features_grad = -2 * error * user_features[u] + beta * item_features[i]\n",
    "\n",
    "            user_features[u] -= alpha * user_features_grad\n",
    "            item_features[i] -= alpha * item_features_grad\n",
    "\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def initialize_shared_arrays(num_users, num_items, num_factors):\n",
    "    global shared_user_features_base, shared_item_features_base\n",
    "    shared_user_features_base = mp.Array('d', num_users * num_factors)\n",
    "    shared_item_features_base = mp.Array('d', num_items * num_factors)\n",
    "    user_features = np.frombuffer(shared_user_features_base.get_obj()).reshape(num_users, num_factors)\n",
    "    item_features = np.frombuffer(shared_item_features_base.get_obj()).reshape(num_items, num_factors)\n",
    "    return user_features, item_features\n",
    "\n",
    "def worker_process(chunk, num_factors, alpha, beta, iteration):\n",
    "    user_features = np.frombuffer(shared_user_features_base.get_obj()).reshape(-1, num_factors)\n",
    "    item_features = np.frombuffer(shared_item_features_base.get_obj()).reshape(-1, num_factors)\n",
    "    for u, i, r in tqdm.tqdm(chunk, desc=f'Iteration {iteration + 1}', position=0, leave=True):\n",
    "        prediction = np.dot(user_features[u], item_features[i])\n",
    "        error = r - prediction\n",
    "\n",
    "        user_features_grad = -2 * error * item_features[i] + beta * user_features[u]\n",
    "        item_features_grad = -2 * error * user_features[u] + beta * item_features[i]\n",
    "\n",
    "        user_features[u] -= alpha * user_features_grad\n",
    "        item_features[i] -= alpha * item_features_grad\n",
    "\n",
    "def parallel_sgd(user_indices, item_indices, ratings, num_users, num_items, num_factors, alpha, beta, iterations):\n",
    "    user_features, item_features = initialize_shared_arrays(num_users, num_items, num_factors)\n",
    "    chunks = np.array_split(list(zip(user_indices, item_indices, ratings)), mp.cpu_count())\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        print(\"Iteration\", iteration + 1)\n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            pool.starmap(worker_process, [(chunk, num_factors, alpha, beta, iteration) for chunk in chunks])\n",
    "\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 20 factors and alpha= 0.005, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:52<00:00, 57287.55it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:52<00:00, 57247.86it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:54<00:00, 56956.73it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:51<00:00, 57403.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6251790190240952\n",
      "\n",
      "Testing with 20 factors and alpha= 0.005, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:51<00:00, 57408.65it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:50<00:00, 57598.00it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:49<00:00, 57857.52it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:47<00:00, 58303.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6412810617479993\n",
      "\n",
      "Testing with 20 factors and alpha= 0.005, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:47<00:00, 58231.33it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:50<00:00, 57753.37it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:03<00:00, 55171.56it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:40<00:00, 49154.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6540758858017097\n",
      "\n",
      "Testing with 20 factors and alpha= 0.01, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:32<00:00, 50350.71it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:32<00:00, 50443.31it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:30<00:00, 50705.10it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:27<00:00, 51085.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6151278998933138\n",
      "\n",
      "Testing with 20 factors and alpha= 0.01, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:28<00:00, 50938.36it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:26<00:00, 51280.11it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:27<00:00, 51198.33it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:27<00:00, 51190.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6319145930184461\n",
      "\n",
      "Testing with 20 factors and alpha= 0.01, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:26<00:00, 51261.49it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:26<00:00, 51236.21it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:28<00:00, 51075.66it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:17<00:00, 52698.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6472433487650507\n",
      "\n",
      "Testing with 20 factors and alpha= 0.015, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:08<00:00, 54321.18it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:08<00:00, 54325.72it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:09<00:00, 54173.44it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:08<00:00, 54307.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6217423372337008\n",
      "\n",
      "Testing with 20 factors and alpha= 0.015, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:07<00:00, 54464.06it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:48<00:00, 58096.46it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:47<00:00, 58292.12it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:48<00:00, 58148.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6368156584838096\n",
      "\n",
      "Testing with 20 factors and alpha= 0.015, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:49<00:00, 57901.61it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:04<00:00, 54939.83it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:18<00:00, 52533.53it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:18<00:00, 52571.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6526061302130394\n",
      "\n",
      "Testing with 40 factors and alpha= 0.005, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:18<00:00, 52547.39it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52412.15it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:18<00:00, 52623.34it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52492.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6225070291490781\n",
      "\n",
      "Testing with 40 factors and alpha= 0.005, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:20<00:00, 52334.90it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52461.75it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52426.51it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52409.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6396079792257353\n",
      "\n",
      "Testing with 40 factors and alpha= 0.005, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52466.87it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52356.87it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:20<00:00, 52294.35it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:19<00:00, 52468.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6529690029835609\n",
      "\n",
      "Testing with 40 factors and alpha= 0.01, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:20<00:00, 52221.03it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:20<00:00, 52286.97it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:20<00:00, 52307.04it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:20<00:00, 52289.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6119278509014608\n",
      "\n",
      "Testing with 40 factors and alpha= 0.01, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:21<00:00, 52157.98it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:15<00:00, 53072.86it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:01<00:00, 55629.00it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [05:02<00:00, 55462.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6297029761476288\n",
      "\n",
      "Testing with 40 factors and alpha= 0.01, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [05:01<00:00, 55535.83it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [05:02<00:00, 55348.60it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [05:02<00:00, 55472.46it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:58<00:00, 56128.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6458858661727371\n",
      "\n",
      "Testing with 40 factors and alpha= 0.015, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59015.14it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59099.04it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58942.57it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59014.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6187727433396328\n",
      "\n",
      "Testing with 40 factors and alpha= 0.015, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58911.40it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59010.78it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58927.19it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59115.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6342637308800162\n",
      "\n",
      "Testing with 40 factors and alpha= 0.015, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58848.15it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58827.51it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58875.17it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59034.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6511186473091547\n",
      "\n",
      "Testing with 60 factors and alpha= 0.005, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58744.92it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58700.89it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58726.37it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58656.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6219212219599535\n",
      "\n",
      "Testing with 60 factors and alpha= 0.005, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58691.30it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58580.29it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58634.29it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58679.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.638545146099937\n",
      "\n",
      "Testing with 60 factors and alpha= 0.005, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58625.84it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:46<00:00, 58488.24it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:46<00:00, 58555.80it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58653.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6515441028742961\n",
      "\n",
      "Testing with 60 factors and alpha= 0.01, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58739.99it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58658.61it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58858.03it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58874.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6108263399970132\n",
      "\n",
      "Testing with 60 factors and alpha= 0.01, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58672.88it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58603.20it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58737.76it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58630.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6290715076920432\n",
      "\n",
      "Testing with 60 factors and alpha= 0.01, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58735.59it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58693.83it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58743.62it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58683.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6452925383044307\n",
      "\n",
      "Testing with 60 factors and alpha= 0.015, beta= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58698.81it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58690.89it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:46<00:00, 58448.56it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58628.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6176865423892338\n",
      "\n",
      "Testing with 60 factors and alpha= 0.015, beta= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:46<00:00, 58476.66it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58603.94it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58691.02it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:46<00:00, 58531.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6332822822467922\n",
      "\n",
      "Testing with 60 factors and alpha= 0.015, beta= 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58706.07it/s]\n",
      "SGD 2/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58847.94it/s]\n",
      "SGD 3/4: 100%|██████████| 16753799/16753799 [04:45<00:00, 58770.92it/s]\n",
      "SGD 4/4: 100%|██████████| 16753799/16753799 [04:44<00:00, 58798.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.650725691821906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "iterations = 4\n",
    "for num_factors in [20, 40, 60]:  # Different complexities\n",
    "    for alpha in [0.005, 0.01, 0.015]:\n",
    "        for beta in [0.05, 0.1, 0.15]:\n",
    "            print(f\"Testing with {num_factors} factors and alpha= {alpha}, beta= {beta}\")\n",
    "            user_features, item_features = sgd(train_data[0], train_data[1], train_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "            sgd_predictions = predict(user_features, item_features, val_data[0], val_data[1])\n",
    "            sgd_rounded_predictions = round_predictions(sgd_predictions)\n",
    "\n",
    "            truth_ratings = val_data[2]\n",
    "            sgd_mae = calculate_mae(truth_ratings, sgd_rounded_predictions)\n",
    "            print(f\"MAE:\", sgd_mae)\n",
    "            print()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_factors = 30  # Latent factors\n",
    "alpha = 0.0075      # Learning rate\n",
    "beta = 0.125       # Regularization\n",
    "iterations = 1   # Number of iterations\n",
    "\n",
    "# Run SGD\n",
    "sgd_user_features, sgd_item_features = sgd(train_data[0], train_data[1], train_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "sgd_predictions = predict(sgd_user_features, sgd_item_features, val_data[0], val_data[1])\n",
    "sgd_rounded_predictions = round_predictions(sgd_predictions)\n",
    "\n",
    "truth_ratings = val_data[2]\n",
    "sgd_mae = calculate_mae(truth_ratings, sgd_rounded_predictions)\n",
    "print(f\"MAE:\", sgd_mae)\n",
    "#MAE: 0.6496064536022441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble best SGD models and average predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def init_factors(num_factors, size):\n",
    "#    \"\"\" Initialize factors as random normal variables, ensuring small starting values. \"\"\"\n",
    "#    np.random.seed(42)\n",
    "#    return np.random.normal(scale=0.1, size=(size, num_factors))\n",
    "#\n",
    "##def update_factors(fixed_factors, ratings_dict, num_factors, lambda_reg):\n",
    "##    num_entities = fixed_factors.shape[0]\n",
    "##    new_factors = np.zeros_like(fixed_factors)\n",
    "##    \n",
    "##    for i in tqdm(range(num_entities), desc='Updating factors', total=num_entities):\n",
    "##        A = np.zeros((num_factors, num_factors))\n",
    "##        b = np.zeros(num_factors)\n",
    "##        if i in ratings_dict:  # Check if there are ratings for this entity\n",
    "##            for j, rating in ratings_dict[i].items():\n",
    "##                A += np.outer(fixed_factors[j], fixed_factors[j])\n",
    "##                b += rating * fixed_factors[j]\n",
    "##            A += lambda_reg * np.eye(num_factors)\n",
    "##            new_factors[i] = np.linalg.solve(A, b)\n",
    "##        else:\n",
    "##            new_factors[i] = np.zeros(num_factors)  # No ratings, potentially initialize differently\n",
    "##    \n",
    "##    return new_factors\n",
    "#\n",
    "#def update_factors(fixed_factors, ratings_dict, num_factors, lambda_reg):\n",
    "#    num_entities = fixed_factors.shape[0]\n",
    "#    new_factors = np.zeros_like(fixed_factors)\n",
    "#\n",
    "#    for i in tqdm(range(num_entities), desc='Updating factors', total=num_entities):\n",
    "#        A = np.zeros((num_factors, num_factors))\n",
    "#        b = np.zeros(num_factors)\n",
    "#        if i in ratings_dict:  # Ensure 'i' is a valid index for ratings_dict\n",
    "#            for j, rating in ratings_dict[i].items():\n",
    "#                # Ensure 'j' is also within the valid range before accessing\n",
    "#                if j < num_entities:\n",
    "#                    A += np.outer(fixed_factors[j], fixed_factors[j])\n",
    "#                    b += rating * fixed_factors[j]\n",
    "#            A += lambda_reg * np.eye(num_factors)\n",
    "#            new_factors[i] = np.linalg.solve(A, b)\n",
    "#        else:\n",
    "#            new_factors[i] = np.zeros(num_factors)  # Handle entities without ratings\n",
    "#\n",
    "#    return new_factors\n",
    "#\n",
    "#def als(train_data, num_users, num_items, num_factors, lambda_reg, iterations):\n",
    "#    user_factors = init_factors(num_factors, num_users)\n",
    "#    item_factors = init_factors(num_factors, num_items)\n",
    "#\n",
    "#    # Convert training data to a dictionary format for fast access\n",
    "#    user_ratings = {u: {} for u in range(num_users)}\n",
    "#    item_ratings = {i: {} for i in range(num_items)}\n",
    "#    for (u, i), r in train_data:\n",
    "#        user_ratings[u][i] = r\n",
    "#        item_ratings[i][u] = r\n",
    "#\n",
    "#    for iteration in tqdm(range(iterations), desc='ALS Iterations'):\n",
    "#        user_factors = update_factors(item_factors, user_ratings, num_factors, lambda_reg)\n",
    "#        item_factors = update_factors(user_factors, item_ratings, num_factors, lambda_reg)\n",
    "#\n",
    "#    return user_factors, item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 2 factors and lambda_reg= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:11<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7203268190350005\n",
      "\n",
      "Testing with 2 factors and lambda_reg= 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7194435243458098\n",
      "\n",
      "Testing with 2 factors and lambda_reg= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7198851716904052\n",
      "\n",
      "Testing with 3 factors and lambda_reg= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7212653196422657\n",
      "\n",
      "Testing with 3 factors and lambda_reg= 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7174561112951309\n",
      "\n",
      "Testing with 3 factors and lambda_reg= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7178425527216518\n",
      "\n",
      "Testing with 4 factors and lambda_reg= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7257369990062935\n",
      "\n",
      "Testing with 4 factors and lambda_reg= 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:11<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.722369438003754\n",
      "\n",
      "Testing with 4 factors and lambda_reg= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALS Iterations: 100%|██████████| 10/10 [00:11<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7208788782157447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#iterations = 10   # Number of ALS iterations\n",
    "#for num_factors in [2, 3, 4]:  # Different complexities\n",
    "#    for lambda_reg in [0.5, 0.75, 1]:  # Different regularization strengths\n",
    "#        print(f\"Testing with {num_factors} factors and lambda_reg= {lambda_reg}\")\n",
    "#        train_dict = [((u, i), r) for u, i, r in zip(train_data[0], train_data[1], train_data[2])]\n",
    "#        als_user_features, als_item_features = als(train_dict, global_num_users, global_num_items, num_factors, lambda_reg, iterations)\n",
    "#        als_predictions = predict(als_user_features, als_item_features, val_data[0], val_data[1])\n",
    "#        als_rounded_predictions = round_predictions(als_predictions)\n",
    "#\n",
    "#        truth_ratings = val_data[2]\n",
    "#        sgd_als = calculate_mae(truth_ratings, als_rounded_predictions)\n",
    "#        print(f\"MAE:\", sgd_als)\n",
    "#        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating factors: 100%|██████████| 26690/26690 [00:27<00:00, 987.18it/s] \n",
      "Updating factors: 100%|██████████| 26690/26690 [00:30<00:00, 887.98it/s]  \n",
      "ALS Iterations: 100%|██████████| 1/1 [00:57<00:00, 57.11s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 41193 is out of bounds for axis 0 with size 26690",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_dict \u001b[38;5;241m=\u001b[39m [((u, i), r) \u001b[38;5;28;01mfor\u001b[39;00m u, i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_data[\u001b[38;5;241m0\u001b[39m], train_data[\u001b[38;5;241m1\u001b[39m], train_data[\u001b[38;5;241m2\u001b[39m])]\n\u001b[0;32m     10\u001b[0m als_user_features, als_item_features \u001b[38;5;241m=\u001b[39m als(train_dict, global_num_users, global_num_items, num_factors, lambda_reg, iterations)\n\u001b[1;32m---> 11\u001b[0m als_predictions \u001b[38;5;241m=\u001b[39m predict(als_user_features, als_item_features, val_data[\u001b[38;5;241m0\u001b[39m], val_data[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     12\u001b[0m als_rounded_predictions \u001b[38;5;241m=\u001b[39m round_predictions(als_predictions)\n\u001b[0;32m     14\u001b[0m truth_ratings \u001b[38;5;241m=\u001b[39m val_data[\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[1;32mIn[46], line 24\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(user_features, item_features, user_indices, item_indices)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(user_features, item_features, user_indices, item_indices):\n\u001b[1;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mdot(user_features[u], item_features[i]) \u001b[38;5;28;01mfor\u001b[39;00m u, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(user_indices, item_indices)])\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[1;32mIn[46], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(user_features, item_features, user_indices, item_indices):\n\u001b[1;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mdot(user_features[u], item_features[i]) \u001b[38;5;28;01mfor\u001b[39;00m u, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(user_indices, item_indices)])\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[1;31mIndexError\u001b[0m: index 41193 is out of bounds for axis 0 with size 26690"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#\n",
    "#num_factors = 15\n",
    "#lambda_reg = 1\n",
    "#iterations = 1\n",
    "#\n",
    "## Assuming train_data is a list of tuples ((user_id, item_id), rating)\n",
    "#train_dict = [((u, i), r) for u, i, r in zip(train_data[0], train_data[1], train_data[2])]\n",
    "#\n",
    "#als_user_features, als_item_features = als(train_dict, global_num_users, global_num_items, num_factors, lambda_reg, iterations)\n",
    "#als_predictions = predict(als_user_features, als_item_features, val_data[0], val_data[1])\n",
    "#als_rounded_predictions = round_predictions(als_predictions)\n",
    "#\n",
    "#truth_ratings = val_data[2]\n",
    "#als_mae = calculate_mae(truth_ratings, als_rounded_predictions)\n",
    "#print(f\"MAE:\", als_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.45, ALS: 0.55, MAE:   0.715358286408303\n",
      "SGD: 0.45, ALS: 0.525, MAE:   0.7314232085679585\n",
      "SGD: 0.45, ALS: 0.5, MAE:   0.7318648559125538\n",
      "SGD: 0.45, ALS: 0.475, MAE:   0.7861874792977808\n",
      "SGD: 0.45, ALS: 0.45, MAE:   0.8148945566964778\n",
      "SGD: 0.475, ALS: 0.55, MAE:   0.7052556034006846\n",
      "SGD: 0.475, ALS: 0.525, MAE:   0.715358286408303\n",
      "SGD: 0.475, ALS: 0.5, MAE:   0.7318648559125538\n",
      "SGD: 0.475, ALS: 0.475, MAE:   0.7318648559125538\n",
      "SGD: 0.475, ALS: 0.45, MAE:   0.7861874792977808\n",
      "SGD: 0.5, ALS: 0.55, MAE:   0.7046483383018659\n",
      "SGD: 0.5, ALS: 0.525, MAE:   0.7052556034006846\n",
      "SGD: 0.5, ALS: 0.5, MAE:   0.695925803246108\n",
      "SGD: 0.5, ALS: 0.475, MAE:   0.7318648559125538\n",
      "SGD: 0.5, ALS: 0.45, MAE:   0.7324721210113724\n",
      "SGD: 0.525, ALS: 0.55, MAE:   0.7206580545434471\n",
      "SGD: 0.525, ALS: 0.525, MAE:   0.7052556034006846\n",
      "SGD: 0.525, ALS: 0.5, MAE:   0.7052556034006846\n",
      "SGD: 0.525, ALS: 0.475, MAE:   0.7217621729049354\n",
      "SGD: 0.525, ALS: 0.45, MAE:   0.7315888263221817\n",
      "SGD: 0.55, ALS: 0.55, MAE:   0.7369990062934747\n",
      "SGD: 0.55, ALS: 0.525, MAE:   0.7206580545434471\n",
      "SGD: 0.55, ALS: 0.5, MAE:   0.7052556034006846\n",
      "SGD: 0.55, ALS: 0.475, MAE:   0.7052003974826101\n",
      "SGD: 0.55, ALS: 0.45, MAE:   0.7217621729049354\n",
      "\n",
      "SGD MAE:  0.7271723528762283\n",
      "ALS MAE:  0.7217621729049354\n",
      "Weighted MAE:  0.695925803246108\n"
     ]
    }
   ],
   "source": [
    "## weighted predictions\n",
    "#\n",
    "#for weight_sgd in [0.45, 0.475, 0.5, 0.525, 0.55]:\n",
    "#    for weight_als in [0.55, 0.525, 0.5, 0.475, 0.45]:\n",
    "#        #weight_als = 1 - weight_sgd\n",
    "#        weighted_predictions = (weight_sgd * sgd_rounded_predictions) + (weight_als * als_rounded_predictions)\n",
    "#        weighted_rounded_predictions = round_predictions(weighted_predictions)\n",
    "#        weighted_mae = calculate_mae(truth_ratings, weighted_rounded_predictions)\n",
    "#        print(f\"SGD: {weight_sgd}, ALS: {weight_als}, MAE:  \", weighted_mae)\n",
    "#print()\n",
    "#\n",
    "#\n",
    "## SGD and ALS MAEs\n",
    "#print(\"SGD MAE: \", sgd_mae)\n",
    "#print(\"ALS MAE: \", als_mae)\n",
    "#\n",
    "#\n",
    "#weight_sgd = 0.5  # Assume SGD has higher validation accuracy\n",
    "#weight_als = 0.5 # ALS is slightly less accurate\n",
    "#\n",
    "## sgd_predictions and als_predictions are arrays of the same shape containing the predicted ratings\n",
    "#weighted_predictions = (weight_sgd * sgd_rounded_predictions) + (weight_als * als_rounded_predictions)\n",
    "#weighted_rounded_predictions = round_predictions(weighted_predictions)\n",
    "#weighted_mae = calculate_mae(truth_ratings, weighted_rounded_predictions)\n",
    "#\n",
    "## weighted MAE\n",
    "#print(\"Weighted MAE: \", weighted_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set and Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (9430, 3)\n",
      "All data size:  90570\n",
      "Ratio All Data / Test: 0.10411836148835155\n",
      "Ratio Train / Val: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "# import test set\n",
    "\n",
    "# 20M dataset\n",
    "test_dir_20M = '../../data/dataset2/test_20Mwithoutratings.csv'\n",
    "\n",
    "# 100k dataset\n",
    "test_dir_100K = '../../data/dataset1/test_100k_withoutratings.csv'\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "def load_data_np(filepath):\n",
    "    return np.loadtxt(filepath, delimiter=',', skiprows=0, dtype='float32')\n",
    "   \n",
    "# Load the dataset (excluding the header if present)\n",
    "test_data = load_data_np(test_dir_100K)\n",
    "\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"All data size: \", len(all_data[0]))\n",
    "\n",
    "print(\"Ratio All Data / Test:\", len(test_data) / len(all_data[0]))\n",
    "print(\"Ratio Train / Val:\", len(val_data[0]) / len(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_indices = np.array([user_to_norm.get(int(user), -1) for user in test_data[:, 0]])\n",
    "test_item_indices = np.array([item_to_norm.get(int(item), -1) for item in test_data[:, 1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD iterations: 100%|██████████| 10/10 [00:13<00:00,  1.39s/it]\n",
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run SGD on all data\n",
    "#num_factors = 20  # Latent factors\n",
    "#alpha = 0.0075      # Learning rate\n",
    "#beta = 0.125       # Regularization\n",
    "#iterations = 20 \n",
    "\n",
    "num_factors = 20\n",
    "alpha = 0.01\n",
    "beta = 0.02\n",
    "iterations = 10\n",
    "\n",
    "sgd_user_factors, sgd_item_factors = sgd(all_data[0], all_data[1], all_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "sgd_test_predictions = round_predictions(predict(sgd_user_factors, sgd_item_factors, test_user_indices, test_item_indices))\n",
    "\n",
    "# Run ALS on all data\n",
    "#num_factors = 3\n",
    "#lambda_reg = 0.75\n",
    "#iterations = 10\n",
    "\n",
    "# Run ALS on all data\n",
    "num_factors = 2  \n",
    "lambda_reg = 0.5  \n",
    "iterations = 10\n",
    "\n",
    "all_data_dict = [((u, i), r) for u, i, r in zip(all_data[0], all_data[1], all_data[2])]\n",
    "als_user_factors, als_item_factors = als(all_data_dict, global_num_users, global_num_items, num_factors, lambda_reg, iterations=10)\n",
    "als_test_predictions = round_predictions(predict(als_user_factors, als_item_factors, test_user_indices, test_user_indices))\n",
    "\n",
    "# weighted predictions\n",
    "weight_sgd = 0.5  \n",
    "weight_als = 0.5 \n",
    "\n",
    "# sgd_predictions and als_predictions are arrays of the same shape containing the predicted ratings\n",
    "weighted_test_predictions = round_predictions((weight_sgd * sgd_test_predictions) + (weight_als * als_test_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_to_original_ids(predictions, user_indices, item_indices, norm_to_user, norm_to_item):\n",
    "    original_user_ids = [norm_to_user.get(idx) for idx in user_indices]\n",
    "    original_item_ids = [norm_to_item.get(idx) for idx in item_indices]\n",
    "    return np.column_stack((original_user_ids, original_item_ids, predictions))\n",
    "\n",
    "final_predictions = revert_to_original_ids(weighted_test_predictions, test_user_indices, test_item_indices, norm_to_user, norm_to_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 84 4.0]\n",
      " [1 87 3.5]\n",
      " [1 180 4.0]\n",
      " ...\n",
      " [943 653 4.0]\n",
      " [943 673 4.5]\n",
      " [943 936 4.0]]\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions[:, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None values found in user/item ID columns.\n"
     ]
    }
   ],
   "source": [
    "if np.any(final_predictions[:, 0:2] == None):\n",
    "    print(\"None values found in user/item ID columns.\")\n",
    "if np.any(final_predictions[:, 2] == None):\n",
    "    print(\"None values found in prediction column.\")\n",
    "\n",
    "# Before converting types, check and replace None values with a default or drop them\n",
    "for i in range(final_predictions.shape[1]):  # Assuming final_predictions has 3 columns\n",
    "    final_predictions[:, i] = np.where(final_predictions[:, i] == None, -1, final_predictions[:, i])\n",
    "\n",
    "# Now try conversion\n",
    "final_predictions[:, 0:2] = final_predictions[:, 0:2].astype(int)\n",
    "predicted_ratings = final_predictions[:, 2].reshape(-1, 1).astype(float)\n",
    "timestamps = test_data[:, 2].reshape(-1, 1).astype(int)\n",
    "\n",
    "predicted_testset = np.hstack((final_predictions[:, 0:2].astype(int), predicted_ratings, timestamps))\n",
    "\n",
    "path = 'Optional_Submission/results3.csv'\n",
    "#np.savetxt(path, predicted_testset, delimiter=\",\", fmt='%d,%d,%.1f,%d')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soc-cmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
