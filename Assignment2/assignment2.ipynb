{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18615333, 4)\n",
      "(1384930, 3)\n",
      "(1861534, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_dir = '../Specification/D2/train_20M_withratings.csv'\n",
    "test_dir = '../Specification/D2/test_20M_withoutratings.csv'\n",
    "\n",
    "# Load the dataset (excluding the header if present)\n",
    "train_data = np.genfromtxt(train_dir, delimiter=',', dtype='float32',  skip_header=0)\n",
    "test_data = np.genfromtxt(test_dir, delimiter=',',  dtype='float32', skip_header=0)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# split 85:15 to mimmic test set\n",
    "split_index = int(len(train_data) * 0.9)\n",
    "\n",
    "# training and validation sets\n",
    "train_subset = train_data[:split_index]\n",
    "val_data = train_data[split_index:]\n",
    "train_data = train_subset\n",
    "\n",
    "print(val_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - actual_ratings: np.array, the actual ratings.\n",
    "    - predicted_ratings: np.array, the predicted ratings.\n",
    "    \"\"\"\n",
    "    # calculate the absolute error between actual and predicted ratings\n",
    "    abs_err = np.abs(actual - predicted)\n",
    "    \n",
    "    # calculate the mean of these absolute errors\n",
    "    mae = np.mean(abs_err)\n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming user IDs and item IDs are 0-indexed\n",
    "def create_sparse_representation(data):\n",
    "    users = data[:, 0].astype(int)\n",
    "    items = data[:, 1].astype(int)\n",
    "    ratings = data[:, 2]\n",
    "    \n",
    "    # If user/item IDs are not 0-indexed, you might need to reindex them\n",
    "    # This can be done by creating a mapping from old to new indices if necessary\n",
    "    \n",
    "    return users, items, ratings\n",
    "\n",
    "users, items, ratings = create_sparse_representation(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            # Adjust indices to be 0-based\n",
    "            u = int(R[i, 0]) - 1\n",
    "            item = int(R[i, 1]) - 1\n",
    "            r = R[i, 2]\n",
    "            prediction = np.dot(P[u, :], Q[:, item])\n",
    "            e = r - prediction\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            P[u, :] += alpha * (2 * e * Q[:, item] - beta * P[u, :])\n",
    "            Q[:, item] += alpha * (2 * e * P[u, :] - beta * Q[:, item])\n",
    "    return P, Q.T\n",
    "\n",
    "# Adjusting the predict_rating function as well\n",
    "def predict_rating(P, Q, user, item):\n",
    "    # Adjust indices to be 0-based\n",
    "    user -= 1\n",
    "    item -= 1\n",
    "    rating_prediction = np.dot(P[user, :], Q[item, :].T)\n",
    "    \n",
    "    # Ensure the rating is within the range 0.5 to 5.0 and round to the nearest 0.5\n",
    "    rating_prediction = max(0.5, min(5.0, rating_prediction))\n",
    "    rounded_rating = round(rating_prediction * 2) / 2\n",
    "    return rounded_rating\n",
    "\n",
    "val_pred = np.hstack((val_data, np.zeros((len(val_data), 1))))  # Add a column for predRating\n",
    "\n",
    "# Then, populate the predRating column\n",
    "for i in range(len(val_data)):\n",
    "    user, item = int(val_pred[i, 0]), int(val_pred[i, 1])\n",
    "    # Call predict_rating with the user and item, ensuring P and Q are accessible\n",
    "    pred_rating = predict_rating(P, Q, user, item)\n",
    "    val_pred[i, 4] = pred_rating  # Column index 4 for predRating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.389442524283736\n"
     ]
    }
   ],
   "source": [
    "print(calculate_mae(val_pred[:,2],val_pred[:,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "#train: userID, itemID, rating, timestamp\n",
    "#test: userID, itemID, timestamp\n",
    "# 20M dataset\n",
    "\n",
    "train_dir = '../Specification/D2/train_20M_withratings.csv'\n",
    "test_dir = '../Specification/D2/test_20M_withoutratings.csv'\n",
    "\n",
    "# 100k dataset\n",
    "train_dir = '../Specification/D1/train_100k_withratings.csv'\n",
    "test_dir = '../Specification/D1/test_100k_withoutratings.csv'\n",
    "\n",
    "# Load the dataset\n",
    "def load_data_np(filepath):\n",
    "    return np.loadtxt(filepath, delimiter=',', skiprows=0, dtype='float32')\n",
    "   \n",
    "# Load the dataset (excluding the header if present)\n",
    "train_data = load_data_np(train_dir)\n",
    "test_data = load_data_np(test_dir)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# split train data into train and validation sets\n",
    "def split_train_val(data, val_ratio=0.1):\n",
    "    np.random.seed(42)  \n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    val_set_size = int(len(data) * val_ratio)\n",
    "    val_indices = shuffled_indices[:val_set_size]\n",
    "    train_indices = shuffled_indices[val_set_size:]\n",
    "    \n",
    "    return data[train_indices], data[val_indices]\n",
    "\n",
    "train_data, val_data = split_train_val(train_data, val_ratio=0.1)\n",
    "\n",
    "print(val_data.shape)\n",
    "\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_matrices(num_users, num_items, num_factors):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    P = np.random.normal(scale=1./num_factors, size=(num_users + 1, num_factors)).astype(np.float64)  # +1 if indices start at 1\n",
    "    Q = np.random.normal(scale=1./num_factors, size=(num_items + 1, num_factors)).astype(np.float64)  # +1 if indices start at 1\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single thread sgd\n",
    "def sgd(train_data, P, Q, num_factors, alpha, beta, iterations):\n",
    "    Q = Q.T  # Transpose for easier multiplication\n",
    "    for iteration in range(iterations): #tqdm(range(iterations), desc='Training Progress', total=iterations):\n",
    "        np.random.shuffle(train_data)  # Shuffle the ratings to prevent order effects\n",
    "        for user, item, rating, _ in tqdm(train_data, desc=f'{iteration+1}/{iterations}:', total=len(train_data)):\n",
    "            user = int(user) - 1\n",
    "            item = int(item) - 1\n",
    "            prediction = np.dot(P[user], Q[:, item])\n",
    "            e = (rating - prediction)\n",
    "            \n",
    "            # Check for large values\n",
    "            if np.any(np.abs(P[user]) > 1e5) or np.any(np.abs(Q[:, item]) > 1e5):\n",
    "                print(f\"Large values detected at iteration {iteration}, user {user}, item {item}\")\n",
    "\n",
    "            # Update rules for P and Q using gradient descent\n",
    "            update = alpha * (e * Q[:, item] - beta * P[user])\n",
    "            P[user] += update\n",
    "            Q[:, item] += alpha * (e * P[user] - beta * Q[:, item])\n",
    "            \n",
    "            # Check for overflow after update\n",
    "            if np.any(np.isnan(P)) or np.any(np.isinf(P)) or np.any(np.isnan(Q)) or np.any(np.isinf(Q)):\n",
    "                print(f\"Overflow/NaN detected at iteration {iteration}, user {user}, item {item}\")\n",
    "                break\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "def sgd_chunk(args):\n",
    "    ratings, P, Q, num_factors, alpha, beta = args\n",
    "    Q = Q.T\n",
    "    for user, item, rating, _ in ratings:\n",
    "        user = int(user) - 1\n",
    "        item = int(item) - 1\n",
    "        prediction = np.dot(P[user], Q[:, item])\n",
    "        e = (rating - prediction)\n",
    "        update = alpha * (e * Q[:, item] - beta * P[user])\n",
    "        P[user] += update\n",
    "        Q[:, item] += alpha * (e * P[user] - beta * Q[:, item])\n",
    "    return P, Q.T\n",
    "\n",
    "def sgd_parallel(ratings, P, Q, num_factors, alpha, beta, iterations):\n",
    "    np.random.shuffle(ratings)\n",
    "    num_processes = cpu_count()\n",
    "    pool = Pool(num_processes)\n",
    "    chunk_size = len(ratings) // num_processes\n",
    "    for iteration in tqdm(range(iterations), desc='Training Progress', total=iterations):\n",
    "        chunks = [ratings[i:i + chunk_size] for i in range(0, len(ratings), chunk_size)]\n",
    "        results = pool.map(sgd_chunk, [(chunk, P, Q, num_factors, alpha, beta) for chunk in chunks])\n",
    "        P, Q = zip(*results)\n",
    "        P = np.mean(P, axis=0)\n",
    "        Q = np.mean(Q, axis=0)\n",
    "    return P, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "num_users = int(np.max(train_data[:, 0]))  # Assumes user IDs are in the first column\n",
    "num_items = int(np.max(train_data[:, 1]))  # Assumes item IDs are in the second column\n",
    "\n",
    "print(f\"Number of Users: {num_users}, Number of Items: {num_items}\")\n",
    "\n",
    "num_factors = 20  # Number of latent factors\n",
    "alpha = 0.01  # Learning rate\n",
    "beta = 0.01  # Regularization parameter\n",
    "iterations = 10  # Number of SGD iterations\n",
    "workers = 4\n",
    "\n",
    "P, Q = init_matrices(num_users, num_items, num_factors)\n",
    "\n",
    "P, Q = sgd(train_data, P, Q, num_factors, alpha, beta, iterations)\n",
    "#P, Q = sgd_parallel(train_data, P, Q, num_factors, alpha, beta, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_predictions(predictions):\n",
    "    rounded_predictions = np.round(predictions * 2) / 2\n",
    "    return np.clip(rounded_predictions, 0.5, 5.0)\n",
    "\n",
    "def predict_rating(user_index, item_index, P, Q):\n",
    "    user_index -= 1  # Ensure 0-based index for users\n",
    "    item_index -= 1  # Ensure 0-based index for items\n",
    "    \n",
    "    \"\"\" Predicts a rating by the user for the item using the dot product of P and Q. \"\"\"\n",
    "    user_vector = P[user_index]\n",
    "    item_vector = Q[item_index]  # Ensure Q is transposed to align the dimensions\n",
    "    return np.dot(user_vector, item_vector)\n",
    "\n",
    "def generate_predictions(validation_set, P, Q):\n",
    "    predictions = []\n",
    "    for user, item, actual_rating, time in validation_set:\n",
    "        user = int(user)\n",
    "        item = int(item)\n",
    "        predicted_rating = predict_rating(user, item, P, Q)\n",
    "        predictions.append(predicted_rating)\n",
    "    return np.array(predictions)\n",
    "\n",
    "predictions = generate_predictions(val_data, P, Q)\n",
    "rounded_predictions = round_predictions(predictions)\n",
    "truth_ratings = val_data[:, 2]\n",
    "\n",
    "mae = calculate_mae(truth_ratings, rounded_predictions)\n",
    "print(\"MAE: \", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
