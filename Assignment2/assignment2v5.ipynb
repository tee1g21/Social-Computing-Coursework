{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset from database\n",
    "def load_data(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Loaded database\")\n",
    "\n",
    "    c = conn.cursor()\n",
    "    print(\"Fetching data ...\")\n",
    "    c.execute('SELECT UserID, ItemID, Rating FROM example_table')\n",
    "    data = c.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    user_indices = []\n",
    "    item_indices = []\n",
    "    ratings_values = []\n",
    "\n",
    "    max_user_id = 0\n",
    "    max_item_id = 0\n",
    "\n",
    "    for user_id, item_id, rating in data:\n",
    "        user_indices.append(user_id)\n",
    "        item_indices.append(item_id)\n",
    "        ratings_values.append(rating)\n",
    "        #ratings_values.append(int(rating * 2)) # convert ratings to integers\n",
    "        max_user_id = max(max_user_id, user_id)\n",
    "        max_item_id = max(max_item_id, item_id)\n",
    "\n",
    "    user_indices = np.array(user_indices, dtype=np.int32)\n",
    "    item_indices = np.array(item_indices, dtype=np.int32)\n",
    "    #ratings_values = np.array(ratings_values, dtype=np.int32)\n",
    "    ratings_values = np.array(ratings_values, dtype=np.float32)\n",
    "\n",
    "    print(\"Max user id:\", max_user_id)\n",
    "    print(\"Max item id:\", max_item_id)\n",
    "\n",
    "    return user_indices, item_indices, ratings_values, max_user_id, max_item_id\n",
    "\n",
    "def normalize_ids(indices):\n",
    "    unique_ids, inverse_indices = np.unique(indices, return_inverse=True)\n",
    "    id_map = {original_id: idx for idx, original_id in enumerate(unique_ids)}\n",
    "    reverse_map = {idx: original_id for idx, original_id in enumerate(unique_ids)}\n",
    "    num_unique = len(unique_ids)  # The total number of unique indices\n",
    "    return inverse_indices, num_unique, id_map, reverse_map\n",
    "\n",
    "\n",
    "\n",
    "# split into train and validation sets\n",
    "def split_data(user_indices, item_indices, ratings, split_ratio=0.9):\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(ratings))\n",
    "    split_point = int(len(ratings) * split_ratio)\n",
    "    train_idx, val_idx = indices[:split_point], indices[split_point:]\n",
    "    \n",
    "    train_data = (user_indices[train_idx], item_indices[train_idx], ratings[train_idx])\n",
    "    val_data = (user_indices[val_idx], item_indices[val_idx], ratings[val_idx])\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded database\n",
      "Fetching data ...\n",
      "Max user id: 138493\n",
      "Max item id: 26744\n",
      "Train data size: 16753799\n",
      "Validation data size: 1861534\n"
     ]
    }
   ],
   "source": [
    "# impor dataset\n",
    "\n",
    "path_100k = '../../data/dataset1/train_100k.db'\n",
    "path_20M = '../../data/dataset2/train_20M.db'\n",
    "\n",
    "global_user_indices, global_item_indices, global_ratings, global_max_user_id, global_max_item_id = load_data(path_20M)\n",
    "\n",
    "# Normalize user and item indices\n",
    "global_user_indices, global_num_users, user_to_norm, norm_to_user = normalize_ids(global_user_indices)\n",
    "global_item_indices, global_num_items, item_to_norm, norm_to_item = normalize_ids(global_item_indices)\n",
    "\n",
    "all_data = (global_user_indices, global_item_indices, global_ratings)\n",
    "train_data, val_data = split_data(global_user_indices, global_item_indices, global_ratings, split_ratio=0.9)\n",
    "\n",
    "print(\"Train data size:\",train_data[0].size)\n",
    "print(\"Validation data size:\",val_data[0].size)\n",
    "\n",
    "# train_data[0] = user_indices\n",
    "# train_data[1] = item_indices\n",
    "# train_data[2] = ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE and Prediction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE and predict methods\n",
    "def calculate_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - actual_ratings: np.array, the actual ratings.\n",
    "    - predicted_ratings: np.array, the predicted ratings.\n",
    "    \"\"\"\n",
    "    # calculate the absolute error between actual and predicted ratings\n",
    "    abs_err = np.abs(actual - predicted)\n",
    "    \n",
    "    # calculate the mean of these absolute errors\n",
    "    mae = np.mean(abs_err)\n",
    "    \n",
    "    return mae # /2\n",
    "\n",
    "# round prediction to nearest 0.5 in range [0.5, 5]\n",
    "def round_predictions(predictions):\n",
    "    rounded_predictions = np.round(predictions * 2) / 2\n",
    "    return np.clip(rounded_predictions, 0.5, 5.0)\n",
    "    #rounded_predictions = np.round(predictions)\n",
    "    #return np.clip(rounded_predictions, 1, 10)\n",
    "\n",
    "def predict(user_features, item_features, user_indices, item_indices):\n",
    "    predictions = np.array([np.dot(user_features[u], item_features[i]) for u, i in zip(user_indices, item_indices)])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(user_indices, item_indices, ratings, num_users, num_items, num_factors, alpha, beta, iterations):\n",
    "    # Initialize feature matrices\n",
    "    np.random.seed(42)\n",
    "    user_features = np.random.normal(0, 0.1, (num_users, num_factors))\n",
    "    item_features = np.random.normal(0, 0.1, (num_items, num_factors))\n",
    "\n",
    "    # SGD updates (only using training data)\n",
    "    for iteration in range(iterations): #tqdm(range(iterations), desc='SGD iterations', total=iterations):\n",
    "        for u, i, r in tqdm(zip(user_indices, item_indices, ratings), desc=f'SGD {iteration+1}/{iterations}', total=len(ratings)):\n",
    "            prediction = np.dot(user_features[u], item_features[i])\n",
    "            error = r - prediction\n",
    "\n",
    "            # Update rules for features\n",
    "            user_features_grad = -2 * error * item_features[i] + beta * user_features[u]\n",
    "            item_features_grad = -2 * error * user_features[u] + beta * item_features[i]\n",
    "\n",
    "            user_features[u] -= alpha * user_features_grad\n",
    "            item_features[i] -= alpha * item_features_grad\n",
    "\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 60 factors and alpha= 0.005, beta= 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/5: 100%|██████████| 16753799/16753799 [05:24<00:00, 51559.17it/s]\n",
      "SGD 2/5: 100%|██████████| 16753799/16753799 [05:25<00:00, 51529.58it/s]\n",
      "SGD 3/5: 100%|██████████| 16753799/16753799 [05:23<00:00, 51757.13it/s]\n",
      "SGD 4/5: 100%|██████████| 16753799/16753799 [05:24<00:00, 51581.25it/s]\n",
      "SGD 5/5: 100%|██████████| 16753799/16753799 [05:23<00:00, 51757.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6030598957633866\n",
      "\n",
      "Testing with 60 factors and alpha= 0.005, beta= 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/6: 100%|██████████| 16753799/16753799 [05:24<00:00, 51682.07it/s]\n",
      "SGD 2/6: 100%|██████████| 16753799/16753799 [05:23<00:00, 51806.54it/s]\n",
      "SGD 3/6: 100%|██████████| 16753799/16753799 [05:23<00:00, 51774.27it/s]\n",
      "SGD 4/6: 100%|██████████| 16753799/16753799 [05:24<00:00, 51691.97it/s]\n",
      "SGD 5/6: 100%|██████████| 16753799/16753799 [05:24<00:00, 51660.46it/s]\n",
      "SGD 6/6: 100%|██████████| 16753799/16753799 [05:23<00:00, 51847.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5989162701299037\n",
      "\n",
      "Testing with 60 factors and alpha= 0.005, beta= 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/7: 100%|██████████| 16753799/16753799 [05:24<00:00, 51566.63it/s]\n",
      "SGD 2/7: 100%|██████████| 16753799/16753799 [05:24<00:00, 51641.02it/s]\n",
      "SGD 3/7: 100%|██████████| 16753799/16753799 [05:24<00:00, 51628.38it/s]\n",
      "SGD 4/7: 100%|██████████| 16753799/16753799 [05:24<00:00, 51620.49it/s]\n",
      "SGD 5/7: 100%|██████████| 16753799/16753799 [05:24<00:00, 51583.60it/s]\n",
      "SGD 6/7: 100%|██████████| 16753799/16753799 [05:23<00:00, 51721.09it/s]\n",
      "SGD 7/7: 100%|██████████| 16753799/16753799 [05:24<00:00, 51605.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5968792404543779\n",
      "\n",
      "Testing with 60 factors and alpha= 0.005, beta= 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/8: 100%|██████████| 16753799/16753799 [05:24<00:00, 51693.81it/s]\n",
      "SGD 2/8: 100%|██████████| 16753799/16753799 [05:23<00:00, 51835.93it/s]\n",
      "SGD 3/8: 100%|██████████| 16753799/16753799 [05:04<00:00, 55013.00it/s]\n",
      "SGD 4/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54915.33it/s]\n",
      "SGD 5/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54876.24it/s]\n",
      "SGD 6/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54828.28it/s]\n",
      "SGD 7/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54852.84it/s]\n",
      "SGD 8/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54868.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5960490111918451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "beta = 0.01\n",
    "alpha = 0.005\n",
    "num_factors = 60\n",
    "\n",
    "iterations = 4\n",
    "for iterations in [5,6,7,8]:\n",
    "    print(f\"Testing with {num_factors} factors and alpha= {alpha}, beta= {beta}\")\n",
    "    user_features, item_features = sgd(train_data[0], train_data[1], train_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "    sgd_predictions = predict(user_features, item_features, val_data[0], val_data[1])\n",
    "    sgd_rounded_predictions = round_predictions(sgd_predictions)\n",
    "\n",
    "    truth_ratings = val_data[2]\n",
    "    sgd_mae = calculate_mae(truth_ratings, sgd_rounded_predictions)\n",
    "    print(f\"MAE:\", sgd_mae)\n",
    "    print()\n",
    "    gc.collect()\n",
    "    \n",
    "#0.607952366166828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 60 factors and alpha= 0.0075, beta= 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/8: 100%|██████████| 16753799/16753799 [04:46<00:00, 58550.62it/s]\n",
      "SGD 2/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58739.78it/s]\n",
      "SGD 3/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58583.86it/s]\n",
      "SGD 4/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58654.28it/s]\n",
      "SGD 5/8: 100%|██████████| 16753799/16753799 [04:44<00:00, 58876.10it/s]\n",
      "SGD 6/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58737.90it/s]\n",
      "SGD 7/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58601.00it/s]\n",
      "SGD 8/8: 100%|██████████| 16753799/16753799 [04:44<00:00, 58818.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5927603256239209\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD 1/8: 100%|██████████| 16753799/16753799 [04:46<00:00, 58562.61it/s]\n",
      "SGD 2/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58712.41it/s]\n",
      "SGD 3/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58728.13it/s]\n",
      "SGD 4/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58732.49it/s]\n",
      "SGD 5/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58752.93it/s]\n",
      "SGD 6/8: 100%|██████████| 16753799/16753799 [04:45<00:00, 58658.41it/s]\n",
      "SGD 7/8: 100%|██████████| 16753799/16753799 [04:46<00:00, 58523.03it/s]\n",
      "SGD 8/8: 100%|██████████| 16753799/16753799 [04:47<00:00, 58187.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5960490111918451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing with 60 factors and alpha= 0.0075, beta= 0.03\n",
    "#SGD 1/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59057.38it/s]\n",
    "#SGD 2/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59179.09it/s]\n",
    "#SGD 3/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59055.18it/s]\n",
    "#SGD 4/4: 100%|██████████| 16753799/16753799 [04:43<00:00, 59117.32it/s]\n",
    "#MAE: 0.6063026514691647\n",
    "\n",
    "num_factors = 60  # Latent factors\n",
    "alpha = 0.0075      # Learning rate\n",
    "beta = 0.03      # Regularization\n",
    "iterations = 8   # Number of iterations\n",
    "\n",
    "# Run SGD\n",
    "print(f\"Testing with {num_factors} factors and alpha= {alpha}, beta= {beta}\")\n",
    "sgd_user_features, sgd_item_features = sgd(train_data[0], train_data[1], train_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "sgd_predictions = predict(sgd_user_features, sgd_item_features, val_data[0], val_data[1])\n",
    "train_pred2 = round_predictions(sgd_predictions)\n",
    "\n",
    "truth_ratings = val_data[2]\n",
    "sgd_mae = calculate_mae(truth_ratings, train_pred2)\n",
    "print(f\"MAE:\", sgd_mae)\n",
    "print()\n",
    "\n",
    "#Testing with 60 factors and alpha= 0.005, beta= 0.01\n",
    "#SGD 1/8: 100%|██████████| 16753799/16753799 [05:24<00:00, 51693.81it/s]\n",
    "#SGD 2/8: 100%|██████████| 16753799/16753799 [05:23<00:00, 51835.93it/s]\n",
    "#SGD 3/8: 100%|██████████| 16753799/16753799 [05:04<00:00, 55013.00it/s]\n",
    "#SGD 4/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54915.33it/s]\n",
    "#SGD 5/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54876.24it/s]\n",
    "#SGD 6/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54828.28it/s]\n",
    "#SGD 7/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54852.84it/s]\n",
    "#SGD 8/8: 100%|██████████| 16753799/16753799 [05:05<00:00, 54868.92it/s]\n",
    "#MAE: 0.5960490111918451\n",
    "\n",
    "num_factors = 60  # Latent factors\n",
    "alpha = 0.005      # Learning rate\n",
    "beta = 0.01       # Regularization\n",
    "iterations = 8   # Number of iterations\n",
    "\n",
    "# Run SGD\n",
    "print()\n",
    "sgd_user_features, sgd_item_features = sgd(train_data[0], train_data[1], train_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "sgd_predictions = predict(sgd_user_features, sgd_item_features, val_data[0], val_data[1])\n",
    "train_pred3 = round_predictions(sgd_predictions)\n",
    "\n",
    "truth_ratings = val_data[2]\n",
    "sgd_mae = calculate_mae(truth_ratings, train_pred3)\n",
    "print(f\"MAE:\", sgd_mae)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble best SGD models and average predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.45, ALS: 0.55, MAE:   0.715358286408303\n",
      "SGD: 0.45, ALS: 0.525, MAE:   0.7314232085679585\n",
      "SGD: 0.45, ALS: 0.5, MAE:   0.7318648559125538\n",
      "SGD: 0.45, ALS: 0.475, MAE:   0.7861874792977808\n",
      "SGD: 0.45, ALS: 0.45, MAE:   0.8148945566964778\n",
      "SGD: 0.475, ALS: 0.55, MAE:   0.7052556034006846\n",
      "SGD: 0.475, ALS: 0.525, MAE:   0.715358286408303\n",
      "SGD: 0.475, ALS: 0.5, MAE:   0.7318648559125538\n",
      "SGD: 0.475, ALS: 0.475, MAE:   0.7318648559125538\n",
      "SGD: 0.475, ALS: 0.45, MAE:   0.7861874792977808\n",
      "SGD: 0.5, ALS: 0.55, MAE:   0.7046483383018659\n",
      "SGD: 0.5, ALS: 0.525, MAE:   0.7052556034006846\n",
      "SGD: 0.5, ALS: 0.5, MAE:   0.695925803246108\n",
      "SGD: 0.5, ALS: 0.475, MAE:   0.7318648559125538\n",
      "SGD: 0.5, ALS: 0.45, MAE:   0.7324721210113724\n",
      "SGD: 0.525, ALS: 0.55, MAE:   0.7206580545434471\n",
      "SGD: 0.525, ALS: 0.525, MAE:   0.7052556034006846\n",
      "SGD: 0.525, ALS: 0.5, MAE:   0.7052556034006846\n",
      "SGD: 0.525, ALS: 0.475, MAE:   0.7217621729049354\n",
      "SGD: 0.525, ALS: 0.45, MAE:   0.7315888263221817\n",
      "SGD: 0.55, ALS: 0.55, MAE:   0.7369990062934747\n",
      "SGD: 0.55, ALS: 0.525, MAE:   0.7206580545434471\n",
      "SGD: 0.55, ALS: 0.5, MAE:   0.7052556034006846\n",
      "SGD: 0.55, ALS: 0.475, MAE:   0.7052003974826101\n",
      "SGD: 0.55, ALS: 0.45, MAE:   0.7217621729049354\n",
      "\n",
      "SGD MAE:  0.7271723528762283\n",
      "ALS MAE:  0.7217621729049354\n",
      "Weighted MAE:  0.695925803246108\n"
     ]
    }
   ],
   "source": [
    "## weighted predictions\n",
    "#\n",
    "#for weight_sgd in [0.45, 0.475, 0.5, 0.525, 0.55]:\n",
    "#    for weight_als in [0.55, 0.525, 0.5, 0.475, 0.45]:\n",
    "#        #weight_als = 1 - weight_sgd\n",
    "#        weighted_predictions = (weight_sgd * sgd_rounded_predictions) + (weight_als * als_rounded_predictions)\n",
    "#        weighted_rounded_predictions = round_predictions(weighted_predictions)\n",
    "#        weighted_mae = calculate_mae(truth_ratings, weighted_rounded_predictions)\n",
    "#        print(f\"SGD: {weight_sgd}, ALS: {weight_als}, MAE:  \", weighted_mae)\n",
    "#print()\n",
    "#\n",
    "#\n",
    "## SGD and ALS MAEs\n",
    "#print(\"SGD MAE: \", sgd_mae)\n",
    "#print(\"ALS MAE: \", als_mae)\n",
    "#\n",
    "#\n",
    "#weight_sgd = 0.5  # Assume SGD has higher validation accuracy\n",
    "#weight_als = 0.5 # ALS is slightly less accurate\n",
    "#\n",
    "## sgd_predictions and als_predictions are arrays of the same shape containing the predicted ratings\n",
    "#weighted_predictions = (weight_sgd * sgd_rounded_predictions) + (weight_als * als_rounded_predictions)\n",
    "#weighted_rounded_predictions = round_predictions(weighted_predictions)\n",
    "#weighted_mae = calculate_mae(truth_ratings, weighted_rounded_predictions)\n",
    "#\n",
    "## weighted MAE\n",
    "#print(\"Weighted MAE: \", weighted_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set and Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (9430, 3)\n",
      "All data size:  90570\n",
      "Ratio All Data / Test: 0.10411836148835155\n",
      "Ratio Train / Val: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "# import test set\n",
    "\n",
    "# 20M dataset\n",
    "test_dir_20M = '../../data/dataset2/test_20Mwithoutratings.csv'\n",
    "\n",
    "# 100k dataset\n",
    "test_dir_100K = '../../data/dataset1/test_100k_withoutratings.csv'\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "def load_data_np(filepath):\n",
    "    return np.loadtxt(filepath, delimiter=',', skiprows=0, dtype='float32')\n",
    "   \n",
    "# Load the dataset (excluding the header if present)\n",
    "test_data = load_data_np(test_dir_100K)\n",
    "\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"All data size: \", len(all_data[0]))\n",
    "\n",
    "print(\"Ratio All Data / Test:\", len(test_data) / len(all_data[0]))\n",
    "print(\"Ratio Train / Val:\", len(val_data[0]) / len(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_indices = np.array([user_to_norm.get(int(user), -1) for user in test_data[:, 0]])\n",
    "test_item_indices = np.array([item_to_norm.get(int(item), -1) for item in test_data[:, 1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD iterations: 100%|██████████| 10/10 [00:13<00:00,  1.39s/it]\n",
      "ALS Iterations: 100%|██████████| 10/10 [00:12<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run SGD on all data\n",
    "#num_factors = 20  # Latent factors\n",
    "#alpha = 0.0075      # Learning rate\n",
    "#beta = 0.125       # Regularization\n",
    "#iterations = 20 \n",
    "\n",
    "num_factors = 20\n",
    "alpha = 0.01\n",
    "beta = 0.02\n",
    "iterations = 10\n",
    "\n",
    "sgd_user_factors, sgd_item_factors = sgd(all_data[0], all_data[1], all_data[2], global_num_users, global_num_items, num_factors, alpha, beta, iterations)\n",
    "sgd_test_predictions = round_predictions(predict(sgd_user_factors, sgd_item_factors, test_user_indices, test_item_indices))\n",
    "\n",
    "# Run ALS on all data\n",
    "#num_factors = 3\n",
    "#lambda_reg = 0.75\n",
    "#iterations = 10\n",
    "\n",
    "# Run ALS on all data\n",
    "num_factors = 2  \n",
    "lambda_reg = 0.5  \n",
    "iterations = 10\n",
    "\n",
    "all_data_dict = [((u, i), r) for u, i, r in zip(all_data[0], all_data[1], all_data[2])]\n",
    "als_user_factors, als_item_factors = als(all_data_dict, global_num_users, global_num_items, num_factors, lambda_reg, iterations=10)\n",
    "als_test_predictions = round_predictions(predict(als_user_factors, als_item_factors, test_user_indices, test_user_indices))\n",
    "\n",
    "# weighted predictions\n",
    "weight_sgd = 0.5  \n",
    "weight_als = 0.5 \n",
    "\n",
    "# sgd_predictions and als_predictions are arrays of the same shape containing the predicted ratings\n",
    "weighted_test_predictions = round_predictions((weight_sgd * sgd_test_predictions) + (weight_als * als_test_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_to_original_ids(predictions, user_indices, item_indices, norm_to_user, norm_to_item):\n",
    "    original_user_ids = [norm_to_user.get(idx) for idx in user_indices]\n",
    "    original_item_ids = [norm_to_item.get(idx) for idx in item_indices]\n",
    "    return np.column_stack((original_user_ids, original_item_ids, predictions))\n",
    "\n",
    "final_predictions = revert_to_original_ids(weighted_test_predictions, test_user_indices, test_item_indices, norm_to_user, norm_to_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 84 4.0]\n",
      " [1 87 3.5]\n",
      " [1 180 4.0]\n",
      " ...\n",
      " [943 653 4.0]\n",
      " [943 673 4.5]\n",
      " [943 936 4.0]]\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions[:, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None values found in user/item ID columns.\n"
     ]
    }
   ],
   "source": [
    "if np.any(final_predictions[:, 0:2] == None):\n",
    "    print(\"None values found in user/item ID columns.\")\n",
    "if np.any(final_predictions[:, 2] == None):\n",
    "    print(\"None values found in prediction column.\")\n",
    "\n",
    "# Before converting types, check and replace None values with a default or drop them\n",
    "for i in range(final_predictions.shape[1]):  # Assuming final_predictions has 3 columns\n",
    "    final_predictions[:, i] = np.where(final_predictions[:, i] == None, -1, final_predictions[:, i])\n",
    "\n",
    "# Now try conversion\n",
    "final_predictions[:, 0:2] = final_predictions[:, 0:2].astype(int)\n",
    "predicted_ratings = final_predictions[:, 2].reshape(-1, 1).astype(float)\n",
    "timestamps = test_data[:, 2].reshape(-1, 1).astype(int)\n",
    "\n",
    "predicted_testset = np.hstack((final_predictions[:, 0:2].astype(int), predicted_ratings, timestamps))\n",
    "\n",
    "path = 'Optional_Submission/results3.csv'\n",
    "#np.savetxt(path, predicted_testset, delimiter=\",\", fmt='%d,%d,%.1f,%d')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soc-cmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
