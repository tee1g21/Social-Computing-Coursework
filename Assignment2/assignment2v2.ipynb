{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE and predict methods\n",
    "def calculate_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - actual_ratings: np.array, the actual ratings.\n",
    "    - predicted_ratings: np.array, the predicted ratings.\n",
    "    \"\"\"\n",
    "    # calculate the absolute error between actual and predicted ratings\n",
    "    abs_err = np.abs(actual - predicted)\n",
    "    \n",
    "    # calculate the mean of these absolute errors\n",
    "    mae = np.mean(abs_err)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "# round prediction to nearest 0.5 in range [0.5, 5]\n",
    "def round_predictions(predictions):\n",
    "    rounded_predictions = np.round(predictions * 2) / 2\n",
    "    return np.clip(rounded_predictions, 0.5, 5.0)\n",
    "\n",
    "# predict rating for a user-item pair\n",
    "def predict_rating(user_id, item_id, user_factors, item_factors):\n",
    "    user_vector = user_factors[user_id]\n",
    "    item_vector = item_factors[item_id]\n",
    "    return np.dot(user_vector, item_vector)\n",
    "\n",
    "# generate predictions for all user-item pairs in the validation set\n",
    "def generate_predictions(validation_dict, user_factors, item_factors):\n",
    "    predictions = []\n",
    "    actual_ratings = []\n",
    "    for (user_id, item_id), actual_rating in validation_dict.items():\n",
    "        predicted_rating = predict_rating(user_id, item_id, user_factors, item_factors)\n",
    "        if predicted_rating is not None:\n",
    "            predictions.append(predicted_rating)\n",
    "            actual_ratings.append(actual_rating)\n",
    "        else:\n",
    "            predictions.append(np.nan)  # Appending NaN for undefined predictions\n",
    "            actual_ratings.append(actual_rating)\n",
    "    return np.array(predictions), np.array(actual_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num users and items\n",
    "def get_max_users_items(data):\n",
    "    max_user = max(user_id for user_id, item_id in data.keys()) + 1\n",
    "    max_item = max(item_id for user_id, item_id in data.keys()) + 1\n",
    "    return max_user, max_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split method\n",
    "def split_data(data, val_percent=0.1):\n",
    "   \n",
    "    # Convert dictionary keys to a list for easy random selection\n",
    "    all_keys = list(data.keys())\n",
    "    total_size = len(all_keys)\n",
    "    val_size = int(total_size * val_percent)\n",
    "    \n",
    "    # Randomly select keys for the validation set\n",
    "    random.seed(42)\n",
    "    val_keys = set(random.sample(all_keys, val_size))\n",
    "    \n",
    "    # Split the data into training and validation dictionaries\n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    for key in all_keys:\n",
    "        if key in val_keys:\n",
    "            val_data[key] = data[key]\n",
    "        else:\n",
    "            train_data[key] = data[key]\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded database\n",
      "fetching data ...\n",
      "Max user id:  943\n",
      "Max item id:  1682\n",
      "Total data size:  90570\n",
      "Train data size:  81513\n",
      "Validation data size:  9057\n"
     ]
    }
   ],
   "source": [
    "# import database\n",
    "\n",
    "conn = sqlite3.connect('../Specification/D1/train_100k.db') #stores the main 100k test\n",
    "#conn = sqlite3.connect('../Specification/D2/train_20M.db') #stores the main 100k test\n",
    "print(\"loaded database\")\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "#Fetch data\n",
    "print(\"fetching data ...\")\n",
    "c.execute('SELECT UserID, ItemID, Rating FROM example_table')\n",
    "data = c.fetchall()\n",
    "\n",
    "#Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Extract matrix defining data\n",
    "ratings_dict = {}\n",
    "max_user_id = 0\n",
    "max_item_id = 0\n",
    "\n",
    "for user_id, item_id, rating in data:\n",
    "    if rating > 0:  # Assuming we only care about positive ratings\n",
    "        ratings_dict[(user_id, item_id)] = rating\n",
    "        max_user_id = max(max_user_id, user_id)\n",
    "        max_item_id = max(max_item_id, item_id)\n",
    "\n",
    "print(\"Max user id: \", max_user_id)\n",
    "print(\"Max item id: \", max_item_id)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = split_data(ratings_dict, val_percent=0.1)\n",
    "\n",
    "print(\"Total data size: \", len(ratings_dict))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Validation data size: \", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_factors(num_factors, max_id):\n",
    "    # Initialize factor vectors with small random numbers\n",
    "    factors = {}\n",
    "    np.random.seed(42)\n",
    "    for i in range(1, max_id + 1):  # IDs start at 1\n",
    "        factors[i] = np.random.normal(scale=0.05, size=num_factors).astype(np.float64)\n",
    "    return factors\n",
    "\n",
    "def sgd_update(user_factors, item_factors, user_id, item_id, actual_rating, alpha, beta, num_factors):\n",
    "    # Predict the rating    \n",
    "    prediction = np.dot(user_factors[user_id], item_factors[item_id])\n",
    "    error = actual_rating - prediction\n",
    "    \n",
    "    #Calculate updates\n",
    "    user_update = alpha * (error * item_factors[item_id] - beta * user_factors[user_id])\n",
    "    item_update = alpha * (error * user_factors[user_id] - beta * item_factors[item_id])\n",
    "\n",
    "    # Clip gradients to prevent overflow\n",
    "    user_update = np.clip(user_update, -1, 1)\n",
    "    item_update = np.clip(item_update, -1, 1)\n",
    "\n",
    "    user_factors[user_id] += user_update\n",
    "    item_factors[item_id] += item_update\n",
    "    \n",
    "    \n",
    "    # Update factors\n",
    "    #user_factors[user_id] += alpha * (error * item_factors[item_id] - beta * user_factors[user_id])\n",
    "    #item_factors[item_id] += alpha * (error * user_factors[user_id] - beta * item_factors[item_id])\n",
    "    return user_factors, item_factors\n",
    "\n",
    "def matrix_factorization_SGD(train_data, num_factors, alpha, beta, num_epochs):\n",
    "    # Initialize factors\n",
    "    \n",
    "    max_user, max_item = get_max_users_items(train_data)\n",
    "    print(\"Number of users:\", max_user, \", Number of items:\", max_item)\n",
    "    \n",
    "    user_factors = init_factors(num_factors, max_user)\n",
    "    item_factors = init_factors(num_factors, max_item)\n",
    "    \n",
    "    # Perform SGD\n",
    "    for epoch in range(num_epochs):\n",
    "        for (user_id, item_id), rating in tqdm(train_data.items(), desc=f'SGD {epoch+1}/{num_epochs}', total=len(train_data)):\n",
    "            user_factors, item_factors = sgd_update(\n",
    "                user_factors, item_factors, user_id, item_id, rating, alpha, beta, num_factors)\n",
    "    \n",
    "    return user_factors, item_factors\n",
    "\n",
    "#def create_id_mapping(ids):\n",
    "#    \"\"\" Create a mapping from ID to a continuous range of indices starting at 0. \"\"\"\n",
    "#    unique_ids = sorted(set(ids))\n",
    "#    return {id: idx for idx, id in enumerate(unique_ids)}\n",
    "#\n",
    "## Extract all user and item IDs\n",
    "#user_ids = [uid for uid, _ in train_data.keys()]\n",
    "#item_ids = [iid for _, iid in train_data.keys()]\n",
    "#\n",
    "## Create mappings\n",
    "#user_mapping = create_id_mapping(user_ids)\n",
    "#item_mapping = create_id_mapping(item_ids)\n",
    "#\n",
    "#def init_factors(num_factors, size):\n",
    "#    \"\"\" Initialize factor matrices with a specified size. \"\"\"\n",
    "#    np.random.seed(42)\n",
    "#    return np.random.normal(scale=0.05, size=(size, num_factors))\n",
    "#\n",
    "#def sgd_update(user_factors, item_factors, user_idx, item_idx, actual_rating, alpha, beta):\n",
    "#    # Calculate prediction and error using indices\n",
    "#    prediction = np.dot(user_factors[user_idx], item_factors[item_idx])\n",
    "#    error = actual_rating - prediction\n",
    "#    \n",
    "#    # Compute updates directly without clipping\n",
    "#    user_update = alpha * (error * item_factors[item_idx] - beta * user_factors[user_idx])\n",
    "#    item_update = alpha * (error * user_factors[user_idx] - beta * item_factors[item_idx])\n",
    "#\n",
    "#    # Apply updates directly\n",
    "#    user_factors[user_idx] += user_update\n",
    "#    item_factors[item_idx] += item_update\n",
    "#    \n",
    "#\n",
    "#def train_sgd_mini_batches(train_data, num_factors, alpha, beta, num_epochs, batch_size=100000):\n",
    "#    # Initialize user and item factors\n",
    "#    user_factors = init_factors(num_factors, len(user_mapping))\n",
    "#    item_factors = init_factors(num_factors, len(item_mapping))\n",
    "#\n",
    "#    for epoch in range(num_epochs):\n",
    "#        # Shuffle training data at the start of each epoch\n",
    "#        shuffled_data = list(train_data.items())\n",
    "#        np.random.shuffle(shuffled_data)\n",
    "#        # Process each batch\n",
    "#        for i in tqdm(range(0, len(shuffled_data), batch_size), desc=f'SGD {epoch+1}/{num_epochs}', total=len(shuffled_data) // batch_size):\n",
    "#            batch_data = shuffled_data[i:i + batch_size]\n",
    "#            for (user_id, item_id), rating in batch_data:\n",
    "#                user_idx = user_mapping[user_id]\n",
    "#                item_idx = item_mapping[item_id]\n",
    "#                sgd_update(user_factors, item_factors, user_idx, item_idx, rating, alpha, beta)\n",
    "#\n",
    "#    return user_factors, item_factors\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise sgd hyperparameters\n",
    "\n",
    "#iterations = 10\n",
    "#for num_factors in [5, 10, 20]:  # Different complexities\n",
    "#    for alpha in [0.01, 0.02, 0.03]:\n",
    "#        for beta in [0.01, 0.02, 0.03]:\n",
    "#            print(f\"Testing with {num_factors} factors and alpha= {alpha}, beta= {beta}\")\n",
    "#            user_factors, item_factors = matrix_factorization_SGD(train_data, num_factors, alpha, beta, iterations)\n",
    "#            SGD_predictions, truth_ratings = generate_predictions(val_data, user_factors, item_factors)\n",
    "#            SGD_rounded_predictions = round_predictions(SGD_predictions)\n",
    "#\n",
    "#            SGD_mae = calculate_mae(truth_ratings, SGD_rounded_predictions)\n",
    "#            print(\"MAE: \", SGD_mae)\n",
    "#            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 944 , Number of items: 1683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76fb15b4336432a97a1193c6677a4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 1/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62155a31a7b849daacbe8e3264f98d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 2/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245d66fa2bd24d84ab8ff847e7672d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 3/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b8fc0435a24c2eaa8e0eed0fd1166f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 4/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf2d8ac7f5b4d0c9480f86d26b78238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 5/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8637e510a58b4df19554bb23f09412e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 6/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aa7c3b6f54461ba83ca7d6406ff889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 7/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1449e1b4dbba4ea3ba6e9a8ec5909141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 8/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d87d00198e4bc2a8d520871a7985fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 9/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c000543826a443a588dc24f0b639d9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SGD 10/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7797283868830739\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_factors = 20\n",
    "alpha = 0.01\n",
    "beta = 0.2\n",
    "iterations = 10\n",
    "\n",
    "# Run SGD\n",
    "user_factors, item_factors = matrix_factorization_SGD(train_data, num_factors, alpha, beta, iterations)\n",
    "sgd_predictions, truth_ratings = generate_predictions(val_data, user_factors, item_factors)\n",
    "sgd_rounded_predictions = round_predictions(sgd_predictions)\n",
    "\n",
    "sgd_mae = calculate_mae(truth_ratings, sgd_rounded_predictions)\n",
    "print(\"MAE: \", sgd_mae)\n",
    "\n",
    "#print(sgd_rounded_predictions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_factors(num_factors, size):\n",
    "    \"\"\" Initialize factors as random normal variables. \"\"\"\n",
    "    np.random.seed(42)\n",
    "    return np.random.normal(scale=0.1, size=(size, num_factors))\n",
    "\n",
    "def update_factors(fixed_factors, ratings_dict, num_factors, lambda_reg):\n",
    "    num_entities = fixed_factors.shape[0]\n",
    "    new_factors = np.zeros_like(fixed_factors)\n",
    "    \n",
    "    for i in range(num_entities):\n",
    "        A = np.zeros((num_factors, num_factors))\n",
    "        b = np.zeros(num_factors)\n",
    "        for j, rating in ratings_dict.get(i, {}).items():\n",
    "            A += np.outer(fixed_factors[j], fixed_factors[j])\n",
    "            b += rating * fixed_factors[j]\n",
    "        A += lambda_reg * np.eye(num_factors)\n",
    "        new_factors[i] = np.linalg.solve(A, b)\n",
    "    \n",
    "    return new_factors\n",
    "\n",
    "def run_als(train_data, num_factors, lambda_reg, iterations):\n",
    "       \n",
    "    num_users, num_items = get_max_users_items(train_data)\n",
    "    \n",
    "    user_factors = init_factors(num_factors, num_users)\n",
    "    item_factors = init_factors(num_factors, num_items)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        user_ratings = {u: {} for u in range(num_users)}\n",
    "        item_ratings = {i: {} for i in range(num_items)}\n",
    "        for (u, i), r in tqdm(train_data.items(), desc=f'ALS {iteration}/{iterations}', total=len(train_data)):\n",
    "            user_ratings[u][i] = r\n",
    "            item_ratings[i][u] = r\n",
    "        \n",
    "        user_factors = update_factors(item_factors, user_ratings, num_factors, lambda_reg)\n",
    "        item_factors = update_factors(user_factors, item_ratings, num_factors, lambda_reg)\n",
    "\n",
    "    return user_factors, item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise als hyperparameters\n",
    "\n",
    "#iterations = 10   # Number of ALS iterations\n",
    "#for num_factors in [1, 2, 3]:  # Different complexities\n",
    "#    for lambda_reg in [0.3, 0.4, 0.5, 0.6]:  # Different regularization strengths\n",
    "#        print(f\"Testing with {num_factors} factors and lambda_reg= {lambda_reg}\")\n",
    "#        user_factors, item_factors = run_als(train_data, num_factors, lambda_reg, iterations=10)\n",
    "#        ALS_predictions, truth_ratings = generate_predictions(val_data, user_factors, item_factors)\n",
    "#        ALS_rounded_predictions = round_predictions(ALS_predictions)\n",
    "#\n",
    "#        ALS_mae = calculate_mae(truth_ratings, ALS_rounded_predictions)\n",
    "#        print(\"MAE: \", ALS_mae)\n",
    "#        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47d02ff73c244a294314a419ef38d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 0/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3c003ad2af4d41b1241be18afc6ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 1/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba2728093414ec7a8e74644b9d03da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 2/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4bb01e84214850866d0f52beefeeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 3/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214f72cb0605482cab24793f9106aead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 4/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b46686d1ad4332bc437a3ad0e030cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 5/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cf7e645d8b4ba88e67e38e0c7b60f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 6/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb2bc1b560644af8df695e8bbc9deaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 7/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe142564d374950b88cac33adfcae75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 8/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649c0e79fbe74c4ba5232e5f481bc254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ALS 9/10:   0%|          | 0/81513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7118251076515403\n"
     ]
    }
   ],
   "source": [
    "# best hyperparameter run\n",
    "\n",
    "num_factors = 2  # Latent factors\n",
    "lambda_reg = 0.5  # Regularization strength\n",
    "iterations = 10   # Number of ALS iterations\n",
    "\n",
    "user_factors, item_factors = run_als(train_data, num_factors, lambda_reg, iterations)\n",
    "ALS_predictions, truth_ratings = generate_predictions(val_data, user_factors, item_factors)\n",
    "ALS_rounded_predictions = round_predictions(ALS_predictions)\n",
    "\n",
    "ALS_mae = calculate_mae(truth_ratings, ALS_rounded_predictions)\n",
    "print(\"MAE: \", ALS_mae)\n",
    "\n",
    "#print(ALS_rounded_predictions.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.45, ALS: 0.55, MAE:   0.7123771668322845\n",
      "SGD: 0.45, ALS: 0.525, MAE:   0.7574251959810092\n",
      "SGD: 0.45, ALS: 0.5, MAE:   0.7579220492436789\n",
      "SGD: 0.45, ALS: 0.475, MAE:   0.8082146406094733\n",
      "SGD: 0.45, ALS: 0.45, MAE:   0.8380810422877333\n",
      "SGD: 0.475, ALS: 0.55, MAE:   0.7119355194876891\n",
      "SGD: 0.475, ALS: 0.525, MAE:   0.7123771668322845\n",
      "SGD: 0.475, ALS: 0.5, MAE:   0.7578668433256045\n",
      "SGD: 0.475, ALS: 0.475, MAE:   0.7578668433256045\n",
      "SGD: 0.475, ALS: 0.45, MAE:   0.8081042287733246\n",
      "SGD: 0.5, ALS: 0.55, MAE:   0.7103897537816054\n",
      "SGD: 0.5, ALS: 0.525, MAE:   0.7121563431599868\n",
      "SGD: 0.5, ALS: 0.5, MAE:   0.7029921607596334\n",
      "SGD: 0.5, ALS: 0.475, MAE:   0.7578668433256045\n",
      "SGD: 0.5, ALS: 0.45, MAE:   0.7596334327039859\n",
      "SGD: 0.525, ALS: 0.55, MAE:   0.7106657833719775\n",
      "SGD: 0.525, ALS: 0.525, MAE:   0.7121563431599868\n",
      "SGD: 0.525, ALS: 0.5, MAE:   0.7121563431599868\n",
      "SGD: 0.525, ALS: 0.475, MAE:   0.7576460196533068\n",
      "SGD: 0.525, ALS: 0.45, MAE:   0.7579772551617533\n",
      "SGD: 0.55, ALS: 0.55, MAE:   0.7247432924809539\n",
      "SGD: 0.55, ALS: 0.525, MAE:   0.7105553715358286\n",
      "SGD: 0.55, ALS: 0.5, MAE:   0.7121011372419124\n",
      "SGD: 0.55, ALS: 0.475, MAE:   0.7123771668322845\n",
      "SGD: 0.55, ALS: 0.45, MAE:   0.7576460196533068\n",
      "\n",
      "SGD MAE:  0.7797283868830739\n",
      "ALS MAE:  0.7118251076515403\n",
      "Weighted MAE:  0.7029921607596334\n"
     ]
    }
   ],
   "source": [
    "# weighted predictions\n",
    "\n",
    "for weight_sgd in [0.45, 0.475, 0.5, 0.525, 0.55]:\n",
    "    for weight_als in [0.55, 0.525, 0.5, 0.475, 0.45]:\n",
    "        #weight_als = 1 - weight_sgd\n",
    "        weighted_predictions = (weight_sgd * sgd_rounded_predictions) + (weight_als * ALS_rounded_predictions)\n",
    "        weighted_rounded_predictions = round_predictions(weighted_predictions)\n",
    "        weighted_mae = calculate_mae(truth_ratings, weighted_rounded_predictions)\n",
    "        print(f\"SGD: {weight_sgd}, ALS: {weight_als}, MAE:  \", weighted_mae)\n",
    "print()\n",
    "\n",
    "\n",
    "# SGD and ALS MAEs\n",
    "print(\"SGD MAE: \", sgd_mae)\n",
    "print(\"ALS MAE: \", ALS_mae)\n",
    "\n",
    "\n",
    "weight_sgd = 0.5  # Assume SGD has higher validation accuracy\n",
    "weight_als = 0.5 # ALS is slightly less accurate\n",
    "\n",
    "# sgd_predictions and als_predictions are arrays of the same shape containing the predicted ratings\n",
    "weighted_predictions = (weight_sgd * sgd_rounded_predictions) + (weight_als * ALS_rounded_predictions)\n",
    "weighted_rounded_predictions = round_predictions(weighted_predictions)\n",
    "weighted_mae = calculate_mae(truth_ratings, weighted_rounded_predictions)\n",
    "\n",
    "# weighted MAE\n",
    "print(\"Weighted MAE: \", weighted_mae)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (9430, 3)\n",
      "All data size:  90570\n",
      "Ratio All Data / Test: 0.10411836148835155\n",
      "Ratio Train / Val: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "# import test set\n",
    "\n",
    "# 20M dataset\n",
    "test_dir_20M = '../Specification/D2/test_20M_withoutratings.csv'\n",
    "\n",
    "# 100k dataset\n",
    "test_dir_100K = '../Specification/D1/test_100k_withoutratings.csv'\n",
    "\n",
    "# Load the dataset\n",
    "def load_data_np(filepath):\n",
    "    return np.loadtxt(filepath, delimiter=',', skiprows=0, dtype='float32')\n",
    "   \n",
    "# Load the dataset (excluding the header if present)\n",
    "test_data = load_data_np(test_dir_100K)\n",
    "\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"All data size: \", len(ratings_dict))\n",
    "\n",
    "print(\"Ratio All Data / Test:\", len(test_data) / len(ratings_dict))\n",
    "print(\"Ratio Train / Val:\", len(val_data) / len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testset_predicitons(test_data, user_factors, item_factors):\n",
    "    # Initialize an empty list to hold predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Assume test_data is a numpy array with columns: [userID, itemID, timestamp]\n",
    "    for user, item, _ in test_data:\n",
    "        user = int(user)\n",
    "        item = int(item)\n",
    "        prediction = predict_rating(user, item, user_factors, item_factors)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 944 , Number of items: 1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD iterations: 100%|██████████| 10/10 [00:12<00:00,  1.29s/it]\n",
      "ALS iterations: 100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run SGD on all data\n",
    "num_factors = 20\n",
    "alpha = 0.01\n",
    "beta = 0.02\n",
    "iterations = 10\n",
    "\n",
    "sgd_user_factors, sgd_item_factors = matrix_factorization_SGD(ratings_dict, num_factors, alpha, beta, iterations)\n",
    "sgd_test_predictions = round_predictions(generate_testset_predicitons(test_data, sgd_user_factors, sgd_item_factors))\n",
    "\n",
    "# Run ALS on all data\n",
    "num_factors = 2  \n",
    "lambda_reg = 0.5  \n",
    "iterations = 10   \n",
    "\n",
    "als_user_factors, als_item_factors = run_als(ratings_dict, num_factors, lambda_reg, iterations=10)\n",
    "als_test_predictions = round_predictions(generate_testset_predicitons(test_data, als_user_factors, als_item_factors))\n",
    "\n",
    "\n",
    "# weighted predictions\n",
    "weight_sgd = 0.5  \n",
    "weight_als = 0.5 \n",
    "\n",
    "# sgd_predictions and als_predictions are arrays of the same shape containing the predicted ratings\n",
    "weighted_test_predictions = round_predictions((weight_sgd * sgd_test_predictions) + (weight_als * als_test_predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy array with the predicted ratings\n",
    "predicted_testset =  complete_data = np.hstack((\n",
    "    test_data[:, :2],  # UserID and ItemID columns\n",
    "    weighted_test_predictions.reshape(-1, 1),  # Predicted ratings\n",
    "    test_data[:, 2:]))  # Timestamp column\n",
    "\n",
    "path = 'Optional_Submission/test.csv'\n",
    "np.savetxt(path, predicted_testset, delimiter=\",\", fmt='%d,%d,%.1f,%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
