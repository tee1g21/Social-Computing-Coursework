{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00000000e+00 3.00000000e+00 8.81250949e+08]\n",
      " [1.00000000e+00 1.10000000e+01 2.00000000e+00 8.81251577e+08]\n",
      " [1.00000000e+00 9.30000000e+01 4.00000000e+00 8.81251843e+08]\n",
      " [1.00000000e+00 2.22000000e+02 5.00000000e+00 8.81251820e+08]\n",
      " [1.00000000e+00 2.92000000e+02 3.00000000e+00 8.81251911e+08]]\n",
      "Training data size: (81513, 4)\n",
      "Validation data size: (9057, 4)\n",
      "Test data size: (9430, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# dataset locations\n",
    "train_dir = '../Specification/D1/train_100k_withratings.csv'\n",
    "test_dir = '../Specification/D1/test_100k_withoutratings.csv'\n",
    "\n",
    "# Load the datasets\n",
    "# userid, itemid, rating, and timestamp\n",
    "train_data = np.genfromtxt(train_dir, delimiter=',', skip_header=0)\n",
    "\n",
    "#userid, itemid, and timestamp \n",
    "test_data = np.genfromtxt(test_dir, delimiter=',', skip_header=0)\n",
    "\n",
    "#print(train_data.shape, test_data.shape)\n",
    "print(train_data[:5])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# split 80:20 OR 90:10\n",
    "split_index = int(len(train_data) * 0.9)\n",
    "\n",
    "# training and validation sets\n",
    "train_subset = train_data[:split_index]\n",
    "val_data = train_data[split_index:]\n",
    "train_data = train_subset\n",
    "\n",
    "print(f\"Training data size: {train_subset.shape}\")\n",
    "print(f\"Validation data size: {val_data.shape}\")\n",
    "\n",
    "print(f\"Test data size: {test_data.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - actual_ratings: np.array, the actual ratings.\n",
    "    - predicted_ratings: np.array, the predicted ratings.\n",
    "    \"\"\"\n",
    "    # calculate the absolute error between actual and predicted ratings\n",
    "    abs_err = np.abs(actual - predicted)\n",
    "    \n",
    "    # calculate the mean of these absolute errors\n",
    "    mae = np.mean(abs_err)\n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set timestamps equal to correct column\n",
    "timestamps = train_data[:, 3]\n",
    "\n",
    "# min and max timestamps\n",
    "min_time = np.min(timestamps)\n",
    "max_time = np.max(timestamps)\n",
    "\n",
    "# convert seconds to years\n",
    "time_range_years = (max_time - min_time) / (60*60*24*365) \n",
    "\n",
    "# calculate suitable number of bins\n",
    "N = max(int(time_range_years * 12), 1)  # At least 1 bin\n",
    "#N = 10\n",
    "\n",
    "# edges for each bin\n",
    "bin_edges = np.linspace(min_time, max_time, N+1)\n",
    "\n",
    "# add tiemstamps to bins\n",
    "timestamp_bins = np.digitize(timestamps, bin_edges, right=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Matrix shape: (943, 1682, 7)\n",
      "Item-User Matrix shape: (1682, 943, 7)\n"
     ]
    }
   ],
   "source": [
    "# Array dimensions\n",
    "num_users = int(np.max(train_data[:, 0]))\n",
    "num_items = int(np.max(train_data[:, 1]))\n",
    "\n",
    "# Initialize matrices with an additional dimension for timestamp bins\n",
    "user_item_time_matrix = np.zeros((num_users, num_items, N))\n",
    "item_user_time_matrix = np.zeros((num_items, num_users, N))\n",
    "\n",
    "# Populate the user-item matrix with timestamp bins\n",
    "for i, entry in enumerate(train_data):\n",
    "    user_id, item_id, rating, _ = entry\n",
    "    bin_index = timestamp_bins[i] - 1  # Adjusting index if necessary\n",
    "    user_item_time_matrix[int(user_id)-1, int(item_id)-1, bin_index] = rating\n",
    "\n",
    "# The item-user \"matrix\" involves swapping the user and item dimensions but keeping the bin index the same\n",
    "for i, entry in enumerate(train_data):\n",
    "    user_id, item_id, rating, _ = entry\n",
    "    bin_index = timestamp_bins[i] - 1  # Adjusting index if necessary\n",
    "    item_user_time_matrix[int(item_id)-1, int(user_id)-1, bin_index] = rating\n",
    "\n",
    "print(\"User-Item Matrix shape:\", user_item_time_matrix.shape)\n",
    "print(\"Item-User Matrix shape:\", item_user_time_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_3d(matrix):\n",
    "    # Number of time bins\n",
    "    N = matrix.shape[2]\n",
    "    \n",
    "    # Initialize cosine similarity matrix\n",
    "    num_entities = matrix.shape[0]  # Number of users or items\n",
    "    cosine_sim = np.zeros((num_entities, num_entities))\n",
    "    \n",
    "    # Calculate cosine similarity for each time bin and average\n",
    "    for n in range(N):\n",
    "        # Extract the matrix for the current time bin\n",
    "        matrix_bin = matrix[:, :, n]\n",
    "        \n",
    "        # Calculate the dot product\n",
    "        dot_product = np.dot(matrix_bin, matrix_bin.T)\n",
    "        \n",
    "        # Calculate the norm\n",
    "        norm = np.linalg.norm(matrix_bin, axis=1)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        norm[norm == 0] = 1\n",
    "        \n",
    "        # Calculate the outer product of the norms\n",
    "        norm_outer = np.outer(norm, norm)\n",
    "        \n",
    "        # Calculate cosine similarity for this bin\n",
    "        cosine_sim_bin = dot_product / norm_outer\n",
    "        \n",
    "        # Replace any NaN values with 0\n",
    "        cosine_sim_bin = np.nan_to_num(cosine_sim_bin)\n",
    "        \n",
    "        # Accumulate the results\n",
    "        cosine_sim += cosine_sim_bin\n",
    "    \n",
    "    # Average the accumulated similarities\n",
    "    cosine_sim /= N\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "user_time_sim_matrix = cosine_similarity_3d(user_item_time_matrix)\n",
    "item_time_sim_matrix = cosine_similarity_3d(item_user_time_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7490891023517721\n"
     ]
    }
   ],
   "source": [
    "def predict_rating_item_time(user_id, item_id, item_time_sim_matrix, user_item_time_matrix, timestamp, bin_edges, k=9, default_rating=3):\n",
    "    # Determine the time bin for the given timestamp\n",
    "    bin_index = np.digitize(timestamp, bin_edges) - 1  # Adjusting bin index if necessary\n",
    "    \n",
    "    # Validate indices\n",
    "    if user_id >= user_item_time_matrix.shape[0] or item_id >= item_time_sim_matrix.shape[0] or bin_index >= user_item_time_matrix.shape[2]:\n",
    "        return default_rating\n",
    "    \n",
    "    # Get ratings for the user in the identified time bin\n",
    "    user_ratings = user_item_time_matrix[user_id, :, bin_index]\n",
    "    \n",
    "    # Get the similarities for the target item with all other items, averaged over time as before\n",
    "    item_similarities = item_time_sim_matrix[item_id, :]\n",
    "    \n",
    "    # Identify items that have been rated by the user in this time bin\n",
    "    rated_by_user = user_ratings > 0\n",
    "    \n",
    "    # Filter the similarities and ratings for items rated by the user\n",
    "    similarities = item_similarities[rated_by_user]\n",
    "    ratings = user_ratings[rated_by_user]\n",
    "    \n",
    "    # Select the top-k most similar items, if k is specified\n",
    "    if k > 0 and len(similarities) > k:\n",
    "        top_k_indices = np.argsort(similarities)[-k:]\n",
    "        similarities = similarities[top_k_indices]\n",
    "        ratings = ratings[top_k_indices]\n",
    "    \n",
    "    # Compute the weighted sum of ratings\n",
    "    if similarities.size > 0 and np.sum(similarities) > 0:\n",
    "        weighted_sum = np.dot(similarities, ratings)\n",
    "        sum_of_similarities = np.sum(similarities)\n",
    "        predicted_rating = weighted_sum / sum_of_similarities\n",
    "        return np.clip(round(predicted_rating * 2) / 2, 0.5, 5.0)\n",
    "    else:\n",
    "        return default_rating\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for user_id, item_id, _, timestamp in val_data:\n",
    "    user_id, item_id = int(user_id)-1, int(item_id)-1  # Adjusting for 0-based indexing\n",
    "    pred_rating = predict_rating_item_time(user_id, item_id, item_time_sim_matrix, user_item_time_matrix, timestamp, bin_edges)\n",
    "    predictions.append(pred_rating)\n",
    "\n",
    "predictions_array = np.array(predictions).reshape(-1, 1)\n",
    "val_pred_item = np.hstack((val_data, predictions_array))\n",
    "\n",
    "print(calculate_mae(val_pred_item[:,2],val_pred_item[:,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTEMPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Matrix shape: (943, 1682)\n",
      "Item-User Matrix shape: (1682, 943)\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "## user-item matrix\n",
    "\n",
    "# Array dimensions \n",
    "num_users = int(np.max(train_data[:, 0]))  \n",
    "num_items = int(np.max(train_data[:, 1]))  \n",
    "\n",
    "# Initialize matrices\n",
    "user_item_matrix = np.zeros((num_users, num_items))\n",
    "item_user_matrix = np.zeros((num_items, num_users))\n",
    "\n",
    "# Populate the user-item matrix\n",
    "for entry in train_data:\n",
    "    user_id, item_id, rating, _ = entry\n",
    "    user_item_matrix[int(user_id)-1, int(item_id)-1] = rating\n",
    "\n",
    "# The item-user matrix is the transpose of the user-item matrix\n",
    "item_user_matrix = user_item_matrix.T\n",
    "\n",
    "print(\"User-Item Matrix shape:\", user_item_matrix.shape)\n",
    "print(\"Item-User Matrix shape:\", item_user_matrix.shape)\n",
    "print(user_item_matrix[37, 706])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomev\\AppData\\Local\\Temp\\ipykernel_26536\\312973084.py:14: RuntimeWarning: invalid value encountered in divide\n",
      "  cosine_sim = dot_product / norm_outer\n"
     ]
    }
   ],
   "source": [
    "##cosine similarity\n",
    "def cosine_similarity(matrix):\n",
    "\n",
    "    # Calculate the dot product of given matrix and its transpose\n",
    "    dot_product = np.dot(matrix, matrix.T)\n",
    "\n",
    "    # Calculate the norm of each vector in the matrix\n",
    "    norm = np.linalg.norm(matrix, axis=1)\n",
    "\n",
    "    # Calculate the outer product of the norms\n",
    "    norm_outer = np.outer(norm, norm)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = dot_product / norm_outer\n",
    "    # Replace any NaN values with 0\n",
    "    cosine_sim = np.nan_to_num(cosine_sim)\n",
    "\n",
    "    return cosine_sim\n",
    "\n",
    "user_sim_matrix = cosine_similarity(user_item_matrix)\n",
    "item_sim_matrix = cosine_similarity(item_user_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_popular(user_id, iu_matrix, N=10):\n",
    "    # Calculate the sum of ratings for each item\n",
    "    item_popularity = np.sum(iu_matrix, axis=1)\n",
    "\n",
    "    # Get the indices of the top N popular items\n",
    "    popular_items = np.argsort(item_popularity)[-N:]\n",
    "\n",
    "    # If the user has already rated some of the popular items, remove them from the list\n",
    "    rated_items = np.where(user_item_matrix[user_id, :] != 0)[0]\n",
    "    recommendations = [item for item in popular_items if item not in rated_items]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7533399580435023\n"
     ]
    }
   ],
   "source": [
    "def predict_rating_item(user_id, item_id, item_similarity, user_item_matrix, k=5, default_rating=3):\n",
    "      \n",
    "    # Validate indices\n",
    "    if user_id >= user_item_matrix.shape[0] or item_id >= item_similarity.shape[0]:\n",
    "        return default_rating\n",
    "    \n",
    "    # Get all ratings by the user\n",
    "    user_ratings = user_item_matrix[user_id, :]\n",
    "    \n",
    "    # Get the similarities for the target item with all other items\n",
    "    item_similarities = item_similarity[item_id, :]\n",
    "    \n",
    "    # Identify items that have been rated by the user\n",
    "    rated_by_user = user_ratings > 0\n",
    "    \n",
    "    # Filter the similarities and ratings for items rated by the user\n",
    "    similarities = item_similarities[rated_by_user]\n",
    "    ratings = user_ratings[rated_by_user]\n",
    "    \n",
    "    # Select the top-k most similar items, if k is specified\n",
    "    if k > 0:\n",
    "        if len(similarities) > k:\n",
    "            top_k_indices = np.argsort(similarities)[-k:]\n",
    "            similarities = similarities[top_k_indices]\n",
    "            ratings = ratings[top_k_indices]\n",
    "    \n",
    "    # Compute the weighted sum of ratings\n",
    "    if similarities.size > 0 and np.sum(similarities) > 0:\n",
    "        weighted_sum = np.dot(similarities, ratings)\n",
    "        sum_of_similarities = np.sum(similarities)\n",
    "        predicted_rating = weighted_sum / sum_of_similarities\n",
    "        return np.clip(round(predicted_rating * 2) / 2, 0.5, 5.0)\n",
    "    else:\n",
    "        return default_rating\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for user_id, item_id, _, _ in val_data:\n",
    "    user_id, item_id = int(user_id)-1, int(item_id)-1  # Adjusting for 0-based indexing\n",
    "    pred_rating = predict_rating_item(user_id, item_id, item_sim_matrix, user_item_matrix)\n",
    "    predictions.append(pred_rating)\n",
    "\n",
    "predictions_array = np.array(predictions).reshape(-1, 1)\n",
    "val_pred_item = np.hstack((val_data, predictions_array))\n",
    "\n",
    "print(calculate_mae(val_pred_item[:,2],val_pred_item[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7376614773103677\n"
     ]
    }
   ],
   "source": [
    "## user based prediction\n",
    "#                                                                              28                                           0.025\n",
    "def predict_rating_user(user_id, item_id, user_similarity, user_item_matrix, k=22, default_rating=3.0, similarity_threshold=0.025):\n",
    "    # Guard against out-of-bound indices\n",
    "    if user_id >= user_similarity.shape[0] or item_id >= user_item_matrix.shape[1]:\n",
    "        return default_rating\n",
    "    \n",
    "    # Check if the user has rated any items yet\n",
    "    if np.count_nonzero(user_item_matrix[user_id, :]) == 0:\n",
    "        # This is a new user, recommend a popular item\n",
    "        recommendations = recommend_popular(user_id, user_item_matrix.T)\n",
    "        if item_id in recommendations:\n",
    "            # If the item is among the recommended items, return a high rating\n",
    "            return 5\n",
    "        else:\n",
    "            # Otherwise, return the default rating\n",
    "            return default_rating\n",
    "    \n",
    "    # Calculate user biases\n",
    "    user_mean_ratings = np.true_divide(user_item_matrix.sum(1), (user_item_matrix != 0).sum(1))\n",
    "    user_bias = np.nan_to_num(user_mean_ratings - np.mean(user_mean_ratings))  # Subtract global average\n",
    "    \n",
    "    similarities = user_similarity[user_id, :]\n",
    "    ratings = user_item_matrix[:, item_id] - user_bias  # Apply bias correction to ratings\n",
    "    \n",
    "    # Apply similarity threshold and get the k most similar users\n",
    "    valid_indices = (ratings != -user_bias) & (similarities > similarity_threshold)\n",
    "    valid_similarities = similarities[valid_indices]\n",
    "    valid_ratings = ratings[valid_indices]\n",
    "    \n",
    "    # Get the indices of the k most similar users\n",
    "    k_similar_users = np.argsort(-valid_similarities)[:k]\n",
    "    k_similarities = valid_similarities[k_similar_users]\n",
    "    k_ratings = valid_ratings[k_similar_users]\n",
    "    \n",
    "    if k_similarities.size == 0 or np.sum(k_similarities) == 0:\n",
    "        return default_rating\n",
    "    \n",
    "    weighted_sum = np.dot(k_similarities, k_ratings)\n",
    "    sum_of_weights = np.sum(k_similarities)\n",
    "    \n",
    "    predicted_rating = weighted_sum / sum_of_weights + user_bias[user_id]  # Reapply user's bias to prediction\n",
    "    return np.clip(round(predicted_rating * 2) / 2, 0.5, 5.0)\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for user_id, item_id, _, _ in val_data:\n",
    "    user_id, item_id = int(user_id)-1, int(item_id)-1  # Adjusting for 0-based indexing\n",
    "    pred_rating = predict_rating_user(user_id, item_id, user_sim_matrix, user_item_matrix)\n",
    "    predictions.append(pred_rating)\n",
    "\n",
    "predictions_array = np.array(predictions).reshape(-1, 1)\n",
    "val_pred_user = np.hstack((val_data, predictions_array))\n",
    "\n",
    "print(calculate_mae(val_pred_user[:,2],val_pred_user[:,4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6952081263111406\n"
     ]
    }
   ],
   "source": [
    "                                                                                                                 #0.5                   0.5\n",
    "def predict_rating_hybrid(user_id, item_id, user_similarity, item_time_sim_matrix, user_item_matrix, user_item_time_matrix, timestamp, user_based_weight=0.5, item_based_weight=0.5, default_rating=3):\n",
    "    \n",
    "    # Validate indices to avoid out-of-bounds access\n",
    "    if user_id >= user_item_matrix.shape[0] or item_id >= user_item_matrix.shape[1]:\n",
    "        return default_rating\n",
    "    \n",
    "    # Obtain predictions from both models\n",
    "    user_based_rating = predict_rating_user(user_id, item_id, user_similarity, user_item_matrix, default_rating=default_rating)\n",
    "    item_based_rating = predict_rating_item_time(user_id, item_id, item_time_sim_matrix, user_item_time_matrix, timestamp, bin_edges)\n",
    "    \n",
    "    # Calculate the weighted average of the two predictions\n",
    "    weighted_rating = (user_based_weight * user_based_rating + item_based_weight * item_based_rating) / (user_based_weight + item_based_weight)\n",
    "    \n",
    "    return np.clip(round(weighted_rating * 2) / 2, 0.5, 5.0)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for user_id, item_id, _, timestamp in val_data:\n",
    "    user_id, item_id = int(user_id)-1, int(item_id)-1  # Adjusting for 0-based indexing\n",
    "    pred_rating = predict_rating_hybrid(user_id, item_id, user_sim_matrix, item_time_sim_matrix, user_item_matrix, user_item_time_matrix, timestamp)\n",
    "    predictions.append(pred_rating)\n",
    "\n",
    "predictions_array = np.array(predictions).reshape(-1, 1)\n",
    "val_pred_hybrid = np.hstack((val_data, predictions_array))\n",
    "\n",
    "print(calculate_mae(val_pred_hybrid[:,2],val_pred_hybrid[:,4]))\n",
    "\n",
    "\n",
    "# 0.7209\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
